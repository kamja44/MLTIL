{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고차원 공간의 특징\n",
    "- 3차원을 초과하는 고차원 공간의 구조를 상상하기는 매우 어렵다.\n",
    "- 고차원 공간의 경우 두 지점 사이의 거리가 매우 멀어질 수 있으며, 이는 특성 수가 많은 데이터들 사이의 거리가 매우 멀어질 수 있음을 의미한다.\n",
    "\n",
    "차원의 저주\n",
    "- 머신러닝에서 다루는 많은 문제가 수 천에서 수 백만의 특성을 가진 데이터를 다룬다.\n",
    "    - 이는 데이터셋의 특성이 이루는 공간이 수 천에서 수 백만 차원의 공간임을 의미한다.\n",
    "- 수 천에서 수 백만의 차원을 갖는 데이터셋을 훈련시키는 데에는 엄청난 시간과 비용이 발생한다.\n",
    "- 또한, 데이터 사이가 멀기에 새로운 데이터에 대한 예측값을 계산하기 위해 많은 추정 과정을 거처야 한다.\n",
    "    - 따라서 과대적합 위험도가 커지는 등 훈련이 매우 어려워진다.\n",
    "- 데이터 사이의 거리를 충분히 작게 만들기 위해 천문학적 크기의 데이터셋이 요구되기에 사실상 해결이 불가능하다.\n",
    "\n",
    "차원 축소\n",
    "- 고차원의 데이터셋을 저차원의 데이터셋으로 변환하는 과정이다.\n",
    "- 많은 실전 문제의 데이터셋의 특성들이 이루는 차원을 혁신적으로 줄일 수 있다.\n",
    "- 예\n",
    "    - MNIST 데이터셋의 경우 손글씨 이미지의 테두리 영역은 거의 항상 흰색이기에, 숫자 인식을 위해 아무런 역할을 하지 않는다.\n",
    "    - 따라서 그 영역을 제외한 특성들의 차원만을 이용할 수 있다.\n",
    "        - 실제로 784(18*18)개의 픽셀 특성 대신 154개의 픽셀 특성만을 사용해도 숫자 분류를 진행할 수 있음을 아래에서 확인할 수 있다.\n",
    "    \n",
    "장점과 단점\n",
    "- 차원축소를 진행하면, 정보를 일부 또는 상당히 잃게 된다.\n",
    "    - 따라서 적절한 수의 차원으로 축소를 진행해야 하며, 어느 정도로 줄이는 것이 적절한지 알아내야 한다.\n",
    "- 차원축소를 진행하게 되면 훈련 속도가 빨라진다.\n",
    "- 2, 3차원으로 축소할 경우 데이터 시각화가 가능해져서 데이터들의 패턴 인식이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영 기법\n",
    "- 특이값 분해를 이용한 사영 기법을 설명하기 위해 3차우너 공간에 아래와 같이 분포된 60개의 점을 2차우너으로 사영하는 예제를 이용한다.\n",
    "- 언급된 60개의 3차원 데이터는 아래와 같이 (60, 3) 모양의 행렬 X로 생성한다.\n",
    "    - 생성되는 데이터의 x, y좌표는 사인, 코사인 함수를 조합하며, 수학적으로 큰 의미는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "\n",
    "m=60\n",
    "noise=0.1\n",
    "X=np.empty((m, 3))\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise + np.random.randn(m) / 2 # x 좌표\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2 # y 좌표\n",
    "\n",
    "w1, w2 = 0.1, 0.3\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m) # z 좌표 (초평면 + 잡음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA(주성분 분석)\n",
    "- 데이터셋의 주성분은 데이터 샘플들의 분산을 최대로 유지하는 축을 가리키며, 차원의 개수만큼 주성분이 존재한다.\n",
    "- 분산을 최대로 유지하는 축이 첫번째 주성분이며, 이후 축은 이전 축에 수직이면서, 동시에 남은 분산을 최대한 보존하는 축으로 지정한다.\n",
    "- 이 과정을 차원 수 만큼의 주성분을 찾을 때까지 반복한다.\n",
    "\n",
    "차원 축소\n",
    "- 차원축소는 PCA를 통해 찾은 주성분의 일부로 구성된 초평면으로 데이터 샘플을 사용하는 과정을 의미한다.\n",
    "- 첫번째부터 d번째까지의 주성분을 축으로 사용해서 생성되는 d차원의 공간으로 데이터셋이 사영된다.\n",
    "    - 따라서 3차원(3D) 데이터를 2차원(2D) 공간으로 보내려면 예를 들어 아래 그림의 회색 평면을 구성하는 두 개의 축에 해당하는 주성분을 찾아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해(SVD)\n",
    "- 데이터셋의 분산에 대한 주성분은 데이터셋 행렬의 특이값 분해(SVD)를 이용하여 쉽게 구할 수 있다.\n",
    "- 사이킷런의 PCA 모델이 특이값 분해를 이용하여 주성분을 바로 계산할 수 있지만, 아래에선 먼저 특이값 분해를 사용하여 직접 주성분을 구하는 과정을 살펴본다.\n",
    "    - 평균값(mean)이 0이라는 가정하에 특이값 분해를 진행해야 하기에, 아래 코드를 먼저 실행한다.\n",
    "```\n",
    "X_centered = X - X.mean(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해를 진행하는 numpy.linalg.svd() 함수를 이용하여 X_centered에 대하여 특이값 분해를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vt = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 행렬의 모양을 확인하면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"s shape: {s.shape}\")\n",
    "print(f\"Vt shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해가 제대로 이뤄진 것인지는 아래와 같이 검증할 수 있다.\n",
    "- 행렬의 곱셈을 진행하려면, 먼저 s를 (60, 3)모양의 대각행렬로 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(X_centered.shape)\n",
    "S[:3, :3] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터를 이용한 부동소수점 연산은 일반적으로 100% 정확하지 않다.\n",
    "- numpy.allclose() 함수는 지정된 오차범위 안에서 두 부동소수점의 일치여부를 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_centered, U.dot(S).dot(Vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주성분은 앞서 특이값 분해로 얻은 행렬 V의 열 벡터에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원으로 사영하기\n",
    "\n",
    "3차원 공간의 데이터를 2차원으로 보내려면 앞서 구한 2개의 주성분을 선택한 후, 두 축에 의해 생성되는 초평면으로 사영해야 한다.\n",
    "- 2차원으로 사영하려면 행렬 V의 0번과 1번 두 개의 열을 선택한 후, X_centered를 두 개의 주성분으로 구성된 행렬과 곱한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = Vt.T[:, :2] # 주성분 2개 선택\n",
    "X2D = X_centered.dot(w2) # 데이터셋을 2차원으로 사영한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영된 데이터셋을 사이킷런의 PCA 모델을 이용하여 구한 값과 비교를 위해 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D_using_svd = X2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 PCA 모델 활용\n",
    "- 앞서, 특이값 분해를 직접 활용하여 2차원으로 사영하는 과정을 사이킷런의 PCA 모델을 이용하여 간단히 해결할 수 있다.\n",
    "    - 평균값을 0으로 맞추는 과정도 PCA 모델이 알아서 처리한다.\n",
    "- fit()\n",
    "    - 평균값을 0으로 맞춘 후, 특이값 분해를 이용하여 주성분을 찾는다.\n",
    "- transform()\n",
    "    - 찾은 주성분 2개를 이용하여 2차원 공간으로 데이터를 사영하는 과정은 다음과 같다.\n",
    "        - 1. 평균값을 0으로 맞춘다.\n",
    "        - 2. 특이값 분해로 찾은 주성분을 이용하여 2차원으로 사영한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components= 2) # 2차원으로 사영하는 모델\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영된 데이터의 축별 평균은 거의 0이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X2D.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런을 이용하여 PCA를 진행할 때 주성분 축이 반대 방향으로 지정되는 경우가 발생할 수 있다.\n",
    "- 아래의 코드는 그런 일이 벌어졌음을 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(X2D, X2D_using_svd))\n",
    "\n",
    "print(np.allclose(X2D, -X2D_using_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 훈련된 PCA가 알고 있는 주성분과 앞서 특이값 분해로 얻은 주성분이 서로 다른 방향을 가리킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "\n",
    "print(Vt[:2])\n",
    "\n",
    "print(np.allclose(pca.components_, -Vt[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재구성 오차\n",
    "- 아래 코드는 2차원으로 사영된 데이터를 다시 3차원으로 복원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3D_inv = pca.inverse_transform(X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 공간으로 사영되면서, 정보손실이 있었기에 3차우너으로 복원한 값과 원래 3차우너 값 사이에 오차가 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X3D_inv, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재구성 오차는 사영 전과 사영 후 원래 공간으로 복원한 데이터 사이의 평균제곱오차(MSE)로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sum(np.square(X3D_inv - X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverse_transform() 메서드가 내부에서는 사영할 때 사용된 행렬의 전치행렬을 사영된 데이터셋에 곱한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3D_inv_using_svd = X2D_using_svd.dot(Vt[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 구한 inverse_transform() 메서드의 결과와 비교하기 위해, 다시 데이터셋의 평균을 뺀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA 모델의 mean_ 속성에도 동일한 값이 저장되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 값을 비교하면 아래와 같이 일치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명 분산 비율\n",
    "\n",
    "설명 분산 비율은 각 주성분의 축에 대한 데이터셋의 분산 비율을 의미한다.\n",
    "- PCA 객체의 explained_variance_ratio_ 속성에 사영에 사용된 주성분별 설명 분산 비율이 저장되어 있다.\n",
    "- 첫째 주성분의 축에 대한 데이터셋의 분산 비율: 84.25%\n",
    "- 둘째 주성분의 축에 대한 데이터셋의 분산 비율: 14.63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 2차원으로 사영한 결과 0.9% 정도의 분산을 잃는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해로 생성된 행렬 s를 이용하여, 모든 주성분에 대한 설명 분산 비율을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(s) / np.square(s).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 사용된 그래프를 그리는 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차원 공간에서의 2차원 초평면 좌표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0]\n",
    "\n",
    "x1s = np.linspace(axes[0], axes[1], 10)\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x1, x2 = np.meshgrid(x1s, x2s) # x축 좌표\n",
    "\n",
    "C = pca.components_\n",
    "R = C.T.dot(C)\n",
    "z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2]) # z축 좌표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 그리기\n",
    "- 3D 데이터셋, 평면, 그리고 사영 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3.8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 3차원 데이터 산점도 그리기: 초평면 윗쪽, 아랫쪽 데이터 구분\n",
    "X3D_above = X[X[:, 2] > X3D_inv[:, 2]]    # 초평면 윗쪽 데이터\n",
    "X3D_below = X[X[:, 2] <= X3D_inv[:, 2]]   # 초평면 아랫쪽 데이터\n",
    "\n",
    "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"bo\")\n",
    "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"go\", alpha=0.5)\n",
    "\n",
    "# 주성분의 축 그리기\n",
    "# np.linalg.norm(C, axis=0)\n",
    "# ax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"k\"))\n",
    "# ax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"k\"))\n",
    "# ax.plot([0], [0], [0], \"k.\")\n",
    "\n",
    "# 초평면 그리기\n",
    "ax.plot_surface(x1, x2, z, alpha=0.2, color=\"k\")\n",
    "\n",
    "# 초평면으로 사영된 데이터셋 그리기\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k+\")\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k.\")\n",
    "\n",
    "# 사영 과정\n",
    "for i in range(m):\n",
    "    if X[i, 2] > X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"r-\")\n",
    "    else:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"r-\", color=\"#505050\")\n",
    "\n",
    "# 축 레이블 표기\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18, labelpad=10)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18, labelpad=10)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 공간으로 사영된 데이터셋 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k+\")\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k.\")\n",
    "ax.plot([0], [0], \"ko\")\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "ax.axis([-1.5, 1.3, -1.2, 1.2])\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영기법의 한계\n",
    "- 사영기법이 차원축소의 최선이 아닌 경우가 존재한다.\n",
    "- 이어서 설명하는 롤케이크의 경우처럼 데이터셋을 구성하는 저차원 공간이 꼬이거나 뒤집혀 있는 경우, 사영기법을 적용하면 오히려 문제를 어렵게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "롤케이크 그리기\n",
    "- 아래는 롤케이크(스위스 롤) 데이터셋을 생성한다.\n",
    "- X\n",
    "    - 롤케이크 데이터셋(3차원)\n",
    "- t\n",
    "    - 롤케이크 생성에 사용되는 1차원 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# 롤케이크 데이터셋 산점도\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)   # t에 의존한 색상 지정\n",
    "\n",
    "ax.view_init(10, -70)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "롤케이크 사영하기와 펼치기\n",
    "\n",
    "롤케이크 모양의 데이터셋을 2차원으로 사영해도 분류를 깨끗하게 실행할 수 없다.\n",
    "- 사영: x1과 x2를 주성분 축으로 사용한다. 즉, x3을 무시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.5, 4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "ax.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 아래 그림처럼 돌돌 말린 데이터셋을 평평하게 펼치면, 보다 쉽게 분류가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.5, 4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "ax.axis([4, 15, axes[2], axes[3]])\n",
    "plt.ylabel(\"$x2$\", fontsize=18)\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양체 학습\n",
    "- 롤케이크는 3차우너 공간에 존재한다.\n",
    "- 반면, 위 그림에서 보듯 롤케이크를 구성하는 데이터셋 자체는 2차우너 평면을 구부린 형태를 갖는다.\n",
    "- 이처럼 저차우너 공간이 구부리고 꼬인 형태로 보다 고차원의 공간에 위치하면, 그런 저차원 공간을 다양체(manifold)라고 한다.\n",
    "- 즉, 롤케이크는 2차원 다양체가 된다.\n",
    "\n",
    "다양체 학습(manifold learning)\n",
    "- 고차원 공간에 존재하는 데이터셋의 다양체 성질을 학습하는 것을 의미한다.\n",
    "\n",
    "다양체 가정\n",
    "- 고차원의 데이터셋이 대부분 저차원의 다양체와 비슷한 형태를 갖는다 라는 가정을 다양체 가정 또는 다양체 가설이라고 한다.\n",
    "- 다양체 가정과 더불어 저차원의 다양체로 표현할 경우 문제가 보다 쉬워질 것이다 라는 가정도 함께 사용된다.\n",
    "- 앞서 살펴본 롤케이크의 경우가 이 가정을 만족시킨다.\n",
    "    - 하지만, 이 가정또한 항상 성립하지는 않음을 아래 그래프가 잘 보여준다.\n",
    "        - 위쪽 그림\n",
    "            - 3차원 롤케이크 모양의 데이터셋\n",
    "        - 아래쪽 그림\n",
    "            - 돌돌 말린 모양을 펼친 형태의 2차원 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x3s = np.linspace(axes[4], axes[5], 10)\n",
    "x2, x3 = np.meshgrid(x2s, x3s)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = X[:, 0] > 5\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot_wireframe(5, x2, x3, alpha=0.5)\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면, 아래의 경우 저차원 다양체로 말린 부분을 펼치면 분류 문제가 매우 단순해지는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x3s = np.linspace(axes[4], axes[5], 10)\n",
    "x2, x3 = np.meshgrid(x2s, x3s)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = 2 * (t[:] - 4) > X[:, 1]\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.plot([4, 15], [0, 22], \"b-\", linewidth=2)\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA\n",
    "\n",
    "주성분 분석은 가장 많이 사용되는 차원축소 알고리즘이다.\n",
    "주성분 분석의 핵심은 훈련 데이터셋의 분산을 최대한 보존하는 방향으로 차원축소를 진행하는 것이다.\n",
    "\n",
    "분산 보존\n",
    "\n",
    "아래 그림은 분산을 최대로 보존하는 주성분의 의미를 잘 보여준다.\n",
    "- c1\n",
    "    - 첫째 주성분, 이 축으로 사영할 경우 분산이 가장 많이 보존된다.\n",
    "- c2   \n",
    "    - 둘째 주성분, c1과 수직을 이루면서 분산을 가장 많이 보존하는 축\n",
    "\n",
    "2차원 데이터셋이 두 개의 주성분만 존재하며, 첫 번째 주성분이 정해지면 두 번째 주성분은 자동으로 정해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "X = np.random.randn(m, 2) / 10\n",
    "X = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate\n",
    "\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\n",
    "u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n",
    "\n",
    "X_proj1 = X.dot(u1.reshape(-1, 1))\n",
    "X_proj2 = X.dot(u2.reshape(-1, 1))\n",
    "X_proj3 = X.dot(u3.reshape(-1, 1))\n",
    "\n",
    "# 왼편 그래프\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot2grid((3,2), (0, 0), rowspan=3)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "# 오른편 상단 그래프: C1 축으로 사영한 데이터 그래프\n",
    "plt.subplot2grid((3,2), (0, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n",
    "plt.plot(X_proj1[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "# 오른편 중간 그래프: C1과 C2 사이에 있는 파선으로 된 축으로 사영한 데이터 그래프\n",
    "plt.subplot2grid((3,2), (1, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=1)\n",
    "plt.plot(X_proj2[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "# 오른편 하단 그래프: C2 축으로 사영한 데이터 그래프\n",
    "plt.subplot2grid((3,2), (2, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
    "plt.plot(X_proj3[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA 활용: MNIST 데이터 압축\n",
    "\n",
    "\n",
    "fetch_openml() 함수는 지정된 데이터셋과 관련된 다양한 정보를 담은 사전 객체(Bunch 객체)를 반환하며, 특성과 타깃 데이터셋은 각각 다음 키의 값으로 지정되어 있다.\n",
    "- data\n",
    "    - 특성 데이터셋\n",
    "- target\n",
    "    - 타깃 데이터셋\n",
    "\n",
    "특성 데이터셋과 타깃 데이터셋이 판다스의 DataFrame 또는 Series 객체로 저장된다.\n",
    "- 여기서는 각 데이터셋을 넘파이 어레이로 얻기 위해 as_frame=False 옵션을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "\n",
    "print(type(mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 특성 데이터셋과 타깃 데이터셋 넘파이 어레이로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 세트와 테스트 세트로 구분한다.\n",
    "- 훈련 세트\n",
    "    - 전체 데이터셋의 75%, 52500개의 샘플\n",
    "- 테스트 세트\n",
    "    - 나머지 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명 분산 비율과 차원 축소\n",
    "\n",
    "차원 축소를 위한 적절한 차원을 찾기 위해 설명 분산 비율이 95%가 되는 지점까지 몇 개의 주성분이 필요한가를 계산한다.\n",
    "- MNIST 데이터셋의 경우 설명 분산 비율이 95%가 되도록 하려면 154개의 주성분이 필요함이 아래와 같이 확인된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터셋과 관련된 설명 분산 비율과 차원과의 관계를 그래프로 그리면 다음과 같다.\n",
    "- (154, 95) 좌표의 지점 표시\n",
    "- 엘보우\n",
    "    - 설명 분산 비율 그래프의 증가도가 급격하게 떨어지는 지점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)                                # 설명 분산 비율 그래프(파랑색 곡선)\n",
    "\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "\n",
    "plt.plot(d, 0.95, \"ko\")                                     # (154, 95) 좌표의 점\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")                           # 수직 점선\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")                        # 수평 점선\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),      # Elbow 주석\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원을 직접 지정하는 대신 n_components 옵션에 설명 분산 비율을 지정하면 자동으로 해당 분산 비율을 만족하는 최소 차원으로 데이터셋을 사영할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 주성분이 사용되었는가는 n_components_ 속성이 알고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영에 사용된 설명 분산 비율의 합은 아래와 같이 95%를 넘는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재구성 오차\n",
    "- 재구성 오차는 170,000 정도로 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "np.mean(np.sum(np.square(X_recovered - X_train), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 확인한 차원 154를 이용하여 차원축소를 진행하더라도 재구성 오차가 거의 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "np.mean(np.sum(np.square(X_recovered - X_train), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 원본 손글씨 숫자와 재구성된 숫자 이미지를 비교해보면, 화질이 조금 떨어지긴 했지만 숫자 인식에는 전혀 문제가 없다는 것을 확인할 수 있다.\n",
    "- plot_digits() 함수는 여러 개의 손글씨 데이터를 하나로 묶어서 보여준다.\n",
    "\n",
    "단, 한 줄에 5개의 이미지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 손글씨 이미지 총 52,500 훈련 샘플 중 선택된 25개의 이미지를 보여준다.\n",
    "- 각각의 이미지는 2100개 중에 하나씩 선택되었다.\n",
    "    - 52,500 = 25 * 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# 왼편 원본 손글씨 데이터\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "\n",
    "# 오른편 재구성된 손글씨 데이터\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 이어서 설명하는 점진적 PCA 기법의 결과와 비교하기 위해 앞서 설명한 PCA 기법으로 압축된 손글씨 이미지 데이터를 X_reduced_pca로 기억해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca = X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 PCA\n",
    "\n",
    "svd_solver=\"randomized\" 옵션을 사용하면 특이값 분해 알고리즘에 무작위성을 가하여 차원 d만큼의 주성분을 근사적으로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 알고리즘의 시간 복잡도를 비교하면 다음과 같다.\n",
    "- 기존의 특이값 분해 알고리즘의 시간 복잡도\n",
    "    - O(m * n^2) + O(n^3)\n",
    "- 랜덤 특이값 분해 알고리즘의 시간 복잡도\n",
    "    - O(m * d^2) + O(d^3)\n",
    "\n",
    "따라서 d가 n보다 많이 작으면 랜덤 PCA가 훨씬 빠르게 작동한다.\n",
    "이어서 설명하는 점진적 PCA 알고리즘을 소개한 후, 세 알고리즘의 실행속도를 직접 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점진적 PCA\n",
    "\n",
    "주성분 분석 알고리즘은 훈련 세트 전체를 이용한다.\n",
    "따라서 매우 큰 훈련 세트에 대해서는 메모리와 시간과 관련된 문제가 발생한다.\n",
    "이 문제를 해결하기 위해 점진적 PCA(IPCA: Incremental PCA)를 사용할 수 있다.\n",
    "- 미니 배치 학습처럼 지정된 크기의 훈련 데이터(미니 배치)를 차례대로 적재하면서 주성분을 분석한다.\n",
    "- 온라인 PCA를 구현할 때 유용하다.\n",
    "\n",
    "아래 코드는 훈련 세트를 쪼개서 미니 배치 단위로 IncrementalPCA 모델을 훈련하는 방법을 보여준다.\n",
    "- 훈련 세트를 지정된 미니 배치 크기로 쪼갠 후 for 반복문을 실행한다.\n",
    "- 반복 과정에서 fit()가 아닌 partial_fit()를 반드시 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100 # 배치 개수, 미니 배치 크기 = 52,500 / 100 = 525\n",
    "inc_pca = IncrementalPCA(n_components=154) # 154개의 주성분을 사용하는 모델 지정\n",
    "\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    print(\".\", end=\"\") # 학습 진도를 보여주는 용도\n",
    "    inc_pca.partial_fit(X_batch) # partial_fit() 메서드를 사용한다.\n",
    "X_reduced = inc_pca.transform(X_train) # 154 차원으로 사영한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 방식은 아래와 같이 batch_size 옵션인자와 fit() 메서드를 활용하는 방식과 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(X_train) // n_batches\n",
    "\n",
    "inc_pca_fit = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "X_reduced_fit = inc_pca_fit.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 사영 결과가 동일하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_reduced, X_reduced_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재구성 하기\n",
    "점진적 PCA를 이용해서 사영된 데이터를 재구성한 결과를 비교한다.\n",
    "- 숫자 인식에는 문제가 없어 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# 왼쪽\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "\n",
    "# 오른쪽\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered_inc_pca[::2100])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 앞서 설명한 다른 PCA의 결과와 비교하기 위해 점진적 PCA로 사영된 데이터를 기억한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_inc_pca = X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA와 IPCA 비교\n",
    "\n",
    "특이값 분해 알고리즘에 필요한 훈련 세트의 평균값은 동일한 값이 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(pca.mean_, inc_pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 사영 결과는 다르다.\n",
    "- 점진적 PCA 기법의 사영 결과는 상당히 좋기는 하지만 완벽하진 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_reduced_pca, X_reduced_inc_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memmap() 함수 사용하기\n",
    "\n",
    "넘파이의 memmap() 함수를 이용하여 매우 큰 데이터셋을 마치 실제로 메모리에 적재한 것 인양 처리하는 memmap 객체를 생성해서 활용할 수 있다.\n",
    "처리 과정이 좀 복잡하긴 하지만, 몇 개의 지정된 단계를 거치면 된다.\n",
    "- memmap은 memory-map의 줄임말이다.\n",
    "- 실제로는 컴퓨터 저장장치에 저장된 파일을 마치 메모리상의 어레이로 불러온 효과를 보이는 어레이 객체를 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1단계: memmap 객체를 생성한 후 훈련 세트와 연결한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.data\"\n",
    "m, n = X_train.shape\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"write\", shape=(m, n)) # memmap 객체 생성\n",
    "X_mm[:] = X_train # 훈련 세트와 연결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_mm의 자료형은 memmap이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크기는 훈련 세트의 크기와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memmap 객체를 삭제하는 방식으로 더 이상 어레이 조작이 불가능하도록 컴퓨터에 data 확장자를 가진 바이너리 파일로 저장한다.\n",
    "그러면 현재 파이썬이 실행되는 디렉토리에 my_mnist.data라는 파일이 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 동일한 방식으로 저장된 바이너리 파일을 불러온다.\n",
    "    - 어레이의 모양도 다시 동일하게 지정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(\n",
    "    filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기를 지정한 후, IPCA 모델을 훈련시킨다.\n",
    "- 여기서는 fit() 메서드를 그대로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시간 복잡도 비교\n",
    "\n",
    "앞서 소개된 세 종류의 PCA 알고리즘의 시간 복잡도를 예제를 통해 비교한다.\n",
    "- 그러면 랜덤 PCA, 일반 PCA, 점진적 PCA 순으로 점점 느려지는 것을 확인할 수 있다.\n",
    "- 방식은 2차원, 10차원, 154차원으로 사영하는 기준을 따른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for n_components in (2, 10, 154):\n",
    "    print(\"n_components = \", n_components)\n",
    "\n",
    "    # 일반 PCA\n",
    "    regular_pca = PCA(n_components=n_components, svd_solver=\"full\")\n",
    "\n",
    "    # 점진적 PCA\n",
    "    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)\n",
    "    \n",
    "    # 랜덤 PCA\n",
    "    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=\"randomized\")\n",
    "\n",
    "    for name, pca in ((\"PCA\", regular_pca), (\"Inc PCA\", inc_pca), (\"Rnd PCA\", rnd_pca)):\n",
    "        t1 = time.time()\n",
    "        pca.fit(X_train)\n",
    "        t2 = time.time()\n",
    "        print(\"{} : {:.1f} seconds\".format(name, t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 일반 PCA와 랜덤 PCA를 직접 비교한다.\n",
    "비교는 차원을 고정시킨 채 훈련 세트의 크기를 증가시키는 방식이며, 일반 PCA 방식이 훨씬 빠름을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_rpca = []\n",
    "times_pca = []\n",
    "sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]\n",
    "for n_samples in sizes:\n",
    "    X = np.random.randn(n_samples, 5)\n",
    "    \n",
    "    rpca = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
    "    t1 = time.time()\n",
    "    rpca.fit(X)\n",
    "    t2 = time.time()\n",
    "    times_rpca.append(t2 - t1)\n",
    "    \n",
    "    pca = PCA(n_components=2, svd_solver=\"full\")\n",
    "    t1 = time.time()\n",
    "    pca.fit(X)\n",
    "    t2 = time.time()\n",
    "    times_pca.append(t2 - t1)\n",
    "\n",
    "plt.plot(sizes, times_rpca, \"b-o\", label=\"RPCA\")\n",
    "plt.plot(sizes, times_pca, \"r-s\", label=\"PCA\")\n",
    "plt.xlabel(\"n_samples\")\n",
    "plt.ylabel(\"Training time\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"PCA and Randomized PCA time complexity \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커널 PCA\n",
    "\n",
    "기본적으로 선형 분류 또는 회귀만을 지원하는 SVM에 커널 기법을 적용하여 비선형 분류와 회귀를 지원하도록 하는 아이디어를 PCA 기법에 적용한다.\n",
    "\n",
    "커널 기법의 기본 아이디어는 데이터를 고차원으로 보낼 때 갑자기 선형 분류/회귀가 가능해질 수 있다는 것이다.\n",
    "그런데 동일한 이유로 고차원으로 보내진 데이터셋의 주성분 찾기가 보다 수월해 질 수 있다.\n",
    "실제로 롤케이크 데이터의 경우 선형 주성분을 찾는 일이 매우 힘들어 보이지만, 커널 기법을 이용하여 고차원으로 보내질 경우, 선형 주성분을 찾는 일이 가능해짐을 아래 가운데에 있는 그래프를 통해 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 커널 기법 적용 예제\n",
    "\n",
    "아래 코드는 세 종류의 커널을 활용한 결과를 그래프로 보여준다\n",
    "\n",
    "왼쪽\n",
    "- 선형 커널, 일반 PCA와 동일한 효과를 낸다.\n",
    "가운데\n",
    "- RBF 커널\n",
    "오른쪽\n",
    "- 시그모이드 커널"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "\n",
    "lin_pca = KernelPCA(n_components=2, kernel=\"linear\", fit_inverse_transform=True)\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "sig_pca = KernelPCA(n_components=2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
    "\n",
    "y = t> 6.9\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    plt.subplot(subplot)\n",
    "    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"gs\")\n",
    "    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"y^\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 앞서 rbf 커널 기법으로 2차원으로 사영된 데이터셋을 다시 3차원으로 복원한 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "X_inverse = rbf_pca.inverse_transform(X_reduced_rbf)\n",
    "\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "ax.view_init(10, -70)\n",
    "ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker=\"x\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커널 선택과 하이퍼파라미터 조정\n",
    "\n",
    "kPCA는 비지도 학습이다. 따라서 어떤 방식, 어떤 커널이 보다 좋은지 평가할 수 있는 기준이 따로 없다.\n",
    "하지만, 차원축소 이후에 지도학습된 결과를 비교하여 최적의 차원축소 기법과 하이퍼파라미터를 찾을 수는 있다.\n",
    "\n",
    "예를 들어 아래 코드는 그리드 탐색을 이용하여 최적의 커널 기법과 gamma 옵션의 조합을 찾아낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "param_grid=[{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드 탐색 결과 RBF 커널을 gamma=0.043의 조합이 최선임을 알아냈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "찾아낸 최적의 조합을 이용할 때 재구성 오차가 거의 0에 가깝게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced) # 재구성\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(X, X_preimage) # 재구성 오차가 거의 0이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE\n",
    "\n",
    "국소적 선형 임베딩(LLE: Locally Linear Embedding)은 다양한 학습에 의존하는 비선형 차원축소 기법이다.\n",
    "아래 코드는 롤케이크를 LLE 방식으로 2차원으로 사영한 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_reduced = lle.fit_transform(X)\n",
    "\n",
    "plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18)\n",
    "plt.axis([-0.065, 0.055, -0.1, 0.12])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기타 차원축소 기법\n",
    "\n",
    "아래 코드는 기타 차원축소 기법을 사용하는 방식을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사영 기법\n",
    "\n",
    "선형 판별 분석(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_mnist = mnist[\"data\"]\n",
    "y_mnist = mnist[\"target\"]\n",
    "lda.fit(X_mnist, y_mnist)\n",
    "X_reduced_lda = lda.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양체 학습 기법\n",
    "\n",
    "다차원 스케일링(MDS: Multidimensional Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "X_reduced_mds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "X_reduced_isomap = isomap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE(t-distributed Stochastic Neighbor Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 롤케이크를 MDS, Isomap, t-SNE 방식으로 2차원으로 차원축소한 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[\"MDS\", \"Isomap\", \"t-SNE\"]\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "for subplot, title, X_reduced in zip((131, 132, 133), titles,\n",
    "                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
