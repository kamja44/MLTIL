{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지들과 데이터셋을 불러오고, 판다스에서 읽어들여 Target이라는 이름을 갖는 열에 출력 데이터를 넣어준다.\n",
    "california = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df[\"Target\"] = california.target\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준 스케일링을 적용하여 입력 데이터를 정규화한다.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.values[:, :-1])\n",
    "df.values[:, :-1] = scaler.transform(df.values[:, :-1]) # scaler.transform은 scaler.fit으로 학습된 변환을 데이터에 적용하는 부분이다.\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 파이토치 관련 패키지 불러오기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이로 되어 있는 데이터셋을 파이토치 텐서로 변환한다.\n",
    "data = torch.from_numpy(df.values).float()\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1:] # 전체 행을 가져오고, 마지막 열만 가져오는 코드이다. -1은 마지막 열부터 끝까지 선택한다.\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 세팅 값을 선언한다.\n",
    "# 학습률은 이제 필요없으니 주석처리한다.\n",
    "n_epochs = 4000\n",
    "batch_size = 256\n",
    "print_interval = 200\n",
    "#learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential을 활용하여 똑같이 모델을 만든다.\n",
    "model = nn.Sequential( # 순차적으로 레이어를 추가하는 방법이다.\n",
    "    nn.Linear(x.size(-1), 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(4, 3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3, y.size(-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만든 모델을 옵티마이저에 등록한다.\n",
    "# 앞에서는 optim.SGD 클래스를 통해, 모델을 학습하기 위한 옵티마이저 객체를 생성했다.\n",
    "# 이번에는 optim.Adam 클래스를 통해, 아담 옵티마이저를 선언한다.\n",
    "# 이때, 모델 파라미터만 등록하되, 학습률은 인자로 넣어주지 않는 모습을 볼 수 있다.\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이중 for문을 사용하여, 학습을 진행하는 코드이다.\n",
    "\n",
    "# |x| = (total_size, input_dim)\n",
    "# |y| = (total_size, output_dim)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Shuffle the index to feed-forward.\n",
    "    indices = torch.randperm(x.size(0))\n",
    "    x_ = torch.index_select(x, dim=0, index=indices)\n",
    "    y_ = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "    x_ = x_.split(batch_size, dim=0)\n",
    "    y_ = y_.split(batch_size, dim=0)\n",
    "    # |x_[i]| = (batch_size, input_dim)\n",
    "    # |y_[i]| = (batch_size, output_dim)\n",
    "\n",
    "    y_hat = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for x_i, y_i in zip(x_, y_):\n",
    "        # |x_i| = |x_[i]|\n",
    "        # |y_i| = |y_[i]|\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.mse_loss(y_hat_i, y_i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        y_hat += [y_hat_i]\n",
    "\n",
    "    total_loss = total_loss / len(x_)\n",
    "    if (i + 1) % print_interval == 0:\n",
    "        print(\"Epoch %d: loss=%.4e\" % (i + 1, total_loss))\n",
    "\n",
    "y_hat = torch.cat(y_hat, dim=0)\n",
    "y = torch.cat(y_, dim=0)\n",
    "# |y_hat| = (total_size, output_dim)\n",
    "# |y| = (total_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "df = pd.DataFrame(torch.cat([y, y_hat], dim=1).detach().numpy(), columns=[\"y\", \"y_hat\"])\n",
    "sns.pairplot(df, height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
