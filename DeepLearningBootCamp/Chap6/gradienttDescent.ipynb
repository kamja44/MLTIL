{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7179, 0.9239, 0.9498],\n",
      "        [0.9293, 0.1498, 0.9505],\n",
      "        [0.2582, 0.7858, 0.9978]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 랜덤 생성한 텐서의 값이 출력된다.\\ntensor([[0.0700, 0.1105, 0.8782],\\n        [0.3576, 0.5451, 0.1231],\\n        [0.0675, 0.6133, 0.7252]], requires_grad=True)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사하강법을 통해 랜덤하게 생성한 텐서가 특정 텐서 값을 근사계산하도록 파이토치를 구현한다.\n",
    "# 함수의 출력은 목표 텐서와 랜덤 텐서 사이의 차이가 될 것이고, 함수의 입력은 랜덤 생성한 텐서의 현재 값이 될 것이다.\n",
    "# 즉, 랜덤 생성한 텐서의 값을 경사하강법을 활용하여 이리저리 바꿔가며, 함수의 출력값(목표 텐서와 차이 값)을 최소화한다.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 목표 텐서\n",
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])\n",
    "\n",
    "# 랜덤 값을 갖는 텐서 하나를 생성한다.\n",
    "# 랜덤 값을 갖는 텐서는 텐서의 requires_grad 속성이 True가 되도록 설정해야한다.\n",
    "x = torch.rand_like(target) # 이것은 최종 스칼라가 x로 미분된다는 것을 의미한다.\n",
    "x.requires_grad = True # 미분 후, x의 기울기를 구할 수 있다.\n",
    "print(x)\n",
    "''' 랜덤 생성한 텐서의 값이 출력된다.\n",
    "tensor([[0.0700, 0.1105, 0.8782],\n",
    "        [0.3576, 0.5451, 0.1231],\n",
    "        [0.0675, 0.6133, 0.7252]], requires_grad=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1155, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 사이의 손실 값을 계산한다.\n",
    "loss = F.mse_loss(x, target)\n",
    "print(loss) # tensor(0.1155, grad_fn=<MseLossBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' thstlf rkqtdl \\n1-th Loss ; 6.9898e-02\\ntensor([[0.0767, 0.1304, 0.7497],\\n        [0.3671, 0.5351, 0.2291],\\n        [0.2081, 0.6548, 0.7640]], requires_grad=True)\\n2-th Loss ; 4.2284e-02\\ntensor([[0.0819, 0.1458, 0.6498],\\n        [0.3744, 0.5273, 0.3115],\\n        [0.3174, 0.6871, 0.7943]], requires_grad=True)\\n3-th Loss ; 2.5579e-02\\ntensor([[0.0859, 0.1579, 0.5720],\\n        [0.3801, 0.5212, 0.3756],\\n        [0.4024, 0.7122, 0.8178]], requires_grad=True)\\n4-th Loss ; 1.5474e-02\\ntensor([[0.0890, 0.1672, 0.5116],\\n        [0.3845, 0.5165, 0.4255],\\n        [0.4685, 0.7317, 0.8360]], requires_grad=True)\\n5-th Loss ; 9.3607e-03\\ntensor([[0.0915, 0.1745, 0.4646],\\n        [0.3879, 0.5128, 0.4643],\\n        [0.5200, 0.7469, 0.8502]], requires_grad=True)\\n6-th Loss ; 5.6626e-03\\ntensor([[0.0934, 0.1802, 0.4280],\\n        [0.3906, 0.5100, 0.4944],\\n        [0.5600, 0.7587, 0.8613]], requires_grad=True)\\n7-th Loss ; 3.4255e-03\\n...\\n19-th Loss ; 8.2273e-06\\ntensor([[0.0997, 0.1992, 0.3049],\\n        [0.3996, 0.5004, 0.5960],\\n        [0.6947, 0.7984, 0.8985]], requires_grad=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while 반복문을 사용하여, 두 텐서 값의 차이가 변수 threshold의 값보다 작아질 때까지 미분 및 경사하강법을 반복 수행한다.\n",
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    loss.backward() # 계산된 기울기\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "\n",
    "    # 지금은 아래의 두 줄을 이해할 필요가 없다.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    loss = F.mse_loss(x, target)\n",
    "\n",
    "    print('%d-th Loss : %.4e' % (iter_cnt, loss))\n",
    "    print(x)\n",
    "\n",
    "# 가장 주목해야 할 부분은 backward 함수를 통해 편미분을 수행한다는 것이다.\n",
    "# 편미분을 통해 얻어진 그래디언트들이 x.grad에 자동으로 저장되고 이 값을 활용하여 경사하강법을 수행한다.\n",
    "# 참고로 backward를 호출하기 위한 텐서의 크기는 스칼라(scalar)여야 한다.\n",
    "# 만일, 스칼라가 아닌 경우, backward를 호출하면, 파이토치는 오류를 발생시키며 오류의 원인을 알려준다.\n",
    "\n",
    "''' 손실 값이 점차 줄어드는 것을 볼 수 있고, 텐서 x의 값이 목표 텐서 값에 근접해가는 것을 확인할 수 있다.\n",
    "1-th Loss : 6.9898e-02\n",
    "tensor([[0.0767, 0.1304, 0.7497],\n",
    "        [0.3671, 0.5351, 0.2291],\n",
    "        [0.2081, 0.6548, 0.7640]], requires_grad=True)\n",
    "2-th Loss : 4.2284e-02\n",
    "tensor([[0.0819, 0.1458, 0.6498],\n",
    "        [0.3744, 0.5273, 0.3115],\n",
    "        [0.3174, 0.6871, 0.7943]], requires_grad=True)\n",
    "3-th Loss : 2.5579e-02\n",
    "tensor([[0.0859, 0.1579, 0.5720],\n",
    "        [0.3801, 0.5212, 0.3756],\n",
    "        [0.4024, 0.7122, 0.8178]], requires_grad=True)\n",
    "4-th Loss : 1.5474e-02\n",
    "tensor([[0.0890, 0.1672, 0.5116],\n",
    "        [0.3845, 0.5165, 0.4255],\n",
    "        [0.4685, 0.7317, 0.8360]], requires_grad=True)\n",
    "5-th Loss : 9.3607e-03\n",
    "tensor([[0.0915, 0.1745, 0.4646],\n",
    "        [0.3879, 0.5128, 0.4643],\n",
    "        [0.5200, 0.7469, 0.8502]], requires_grad=True)\n",
    "6-th Loss : 5.6626e-03\n",
    "tensor([[0.0934, 0.1802, 0.4280],\n",
    "        [0.3906, 0.5100, 0.4944],\n",
    "        [0.5600, 0.7587, 0.8613]], requires_grad=True)\n",
    "7-th Loss : 3.4255e-03\n",
    "...\n",
    "19-th Loss : 8.2273e-06\n",
    "tensor([[0.0997, 0.1992, 0.3049],\n",
    "        [0.3996, 0.5004, 0.5960],\n",
    "        [0.6947, 0.7984, 0.8985]], requires_grad=True)\n",
    "'''\n",
    "# 만일, 학습률 변수를 조절한다면 텐서 x가 목표 텐서에 근접해 가는 속도가 달라질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 파이토치는 오토그래드(AutoGrad)라는 자동 미분 기능을 제공한다.\n",
    "# 파이토치는 requires_grad 속성이 True인 텐서의 연산을 추적하기 위한 계산 그래프(computation graph)를 구축하고, backward함수가 호출되면 이 그래프를 따라 미분을 자동으로 수행하고, 계산된 그래디언트를 채워 놓는다.\n",
    "# requires_grad의 속성값의 디폴트 값은 False이며, 다음과 같이 텐서의 requires_grad 속성을 True로 만들 수 있다.\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]]).requires_grad_(True)\n",
    "# requires_grad 속성이 True인 텐서가 있을 때 이 텐서가 들어간 연산의 결과가 담긴 텐서도 자동으로 requires_grad 속성값을 True로 갖게 된다.\n",
    "# 다음 코드와 같이 여러 가지 연산을 수행했을 때의 결과 텐서들은 모두 requires_grad 속성값을 True로 갖게 된다.\n",
    "x1 = x + 2\n",
    "print(x1) # tensor([[3., 4.],[5., 6.]], grad_fn=<AddBackward0>)\n",
    "x2 = x - 2\n",
    "print(x2) # tensor([[-1.,  0.],[ 1.,  2.]], grad_fn=<SubBackward0>)\n",
    "x3 = x1 * x2\n",
    "print(x3) # tensor([[-3.,  0.],[ 5., 12.]], grad_fn=<MulBackward0>)\n",
    "y = x3.sum()\n",
    "print(y) # tensor(14., grad_fn=<SumBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
