{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 곱\n",
    "# 곱셈의 앞 행렬 A의 행(row)의 요소들을 행렬 B의 열(column)의 요소들에 각각 곱한 후, 더한 값을 결과 행렬의 요소로 결정하게 된다.\n",
    "# 주의할 점은, 계산 과정 때문에 A의 열의 개수와 B의 행의 개수는 같아야 한다.\n",
    "# 즉, 두 값(A의 열의 개수와 B의 행의 개수)이 다르다면 행렬곱 연산을 수행할 수 없다.\n",
    "# 이러한 행렬의 곱셈 과정을 내적(inner product) 또는, 닷 프로덕트(dot product)라고 한다.\n",
    "\n",
    "# 벡터 행렬 곱\n",
    "# 벡터와 행렬의 곱셈도 행렬의 곱셈처럼 생각할 수 있따.\n",
    "# 주의할 점은, 벡터가 곱셈의 앞에 위치할 경우, 전치(transpose)를 통해 행과 열을 바꿔 표현하여, 곱셈을 수행한다는 것이다.\n",
    "# 같은 연산 과정을 벡터와 행렬의 위치를 바꿔 표현할 수 있다.\n",
    "# 이 경우에는 곱셈 앞에 기존 행렬 대신, 전치 행렬을 구하여 연산에 투입하는 것에 대해 유의해야한다.\n",
    "# 이 경우, 이전 벡터 행렬 곱셈의 결과에 전치 연산을 수행한 것과 다른 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2, 2])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 파이토치에서 행렬 곱을 구현한다.\n",
    "import torch\n",
    "x = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])\n",
    "y = torch.FloatTensor([[1, 2], [1, 2]])\n",
    "\n",
    "print(x.size(), y.size()) # torch.Size([3, 2]) torch.Size([2, 2])\n",
    "\n",
    "# 파이토치의 matmul 함수를 이용하여, 행렬 곱을 수행할 수 있다.\n",
    "z = torch.matmul(x, y)\n",
    "print(z.size()) # torch.Size([3, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 배치 행렬 곱(실습)\n",
    "# 딥러닝을 수행할 때 보통 여러 샘플을 동시에 병렬 계산하곤 한다.\n",
    "# 따라서, 행렬 곱 연산의 경우에도 여러 곱셈을 동시에 진행할 수 있어야 한다.\n",
    "# bmm(Batch Matrix Multiplication) 함수가 이 역할을 한다.\n",
    "x = torch.FloatTensor(3, 3, 2) # 3 x 3 x 2 크기의 텐서는 3 x 2 크기의 행렬이 3개 있는 것으로 판단할 수 있다.\n",
    "y = torch.FloatTensor(3, 2, 3) # 3 x 2 x 3 크기의 텐서는 2 x 3 크기의 행렬이 3개 있는 것으로 판단할 수 있다.\n",
    "# 즉, bmm 함수를 활용하여 행렬 곱이 3번 수행되는 연산을 병렬로 동시에 진행할 수 있다.\n",
    "z = torch.bmm(x, y)\n",
    "print(z.size()) # torch.Size([3, 3, 3])\n",
    "# 결과물의 크기가 예상한 대로 3 x 3 크기의 행렬이 3개 있는 3 x 3 x 3 형태로 나오는 것을 볼 수 있따.\n",
    "# 이처럼, bmm 함수는 마지막 2개의 차원을 행렬 취급하여, 병렬로 행렬 곱 연산을 수행한다.\n",
    "# bmm 함수를 적용하기 위해선, 마지막 2개의 차원을 제외한 다른 차원의 크기는 동일해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 계층\n",
    "# 선형 계층은 뒤에서 다룰 심층신경망(deep neural networks)의 가장 기본적인 구성요소이다.\n",
    "# 그뿐만 아니라, 하나의 모델로도 동작할 수 있다.\n",
    "# 또한, 선형 계층은 하나의 함수로 볼 수 있다.\n",
    "# 함수는 가중치 파라미터(weight parameter)를 가지고 있으며, 이것에 의해 함수의 동작이 정의된다.\n",
    "# 출력 노드의 값은, 입력 노드로부터 들어오는 값에 가중치 파라미터(W)를 곱하고, 또 다른 가중치 파라미터 b를 더해서 결정한다.\n",
    "\n",
    "# 미니 배치\n",
    "# 만일 수백만개의 입력 벡터가 주어졌다고 했을 때, 단순히 순차적으로 처리한다면 매우 비효율적일 것이다.\n",
    "# 즉, 이 연산을 다수의 입력을 처리하기 위한 병렬(parallel) 연산으로 생각해 볼 수 있다.\n",
    "# N개의 n차원 벡터를 모아 N x n 크기의 행렬로 만들 수 있다.\n",
    "# 이것을 미니배치라고 한다.\n",
    "# 입력을 N개 모아서 미니배치 행렬로 넣어주면, 출력도 N개의 m차원 벡터가 모여 N x m 크기의 행렬이 된다.\n",
    "\n",
    "# 선형 계층의 의미\n",
    "# 선형 계층은 행렬 곱셈과 벡터의 덧셈으로 이뤄졌기에 선형 변환이라고 볼 수 있다.\n",
    "# 즉, 선형 계층을 통해 모델을 구성할 경우, 선형 데이터에 대한 관계를 분석하거나 선형 함수를 근사계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 선형 계층(실습)\n",
    "# 선형 계층은 행렬 곱 연산과 브로드캐스팅 덧셈 연산으로 이뤄져 있따.\n",
    "# 선형 계층의 파라미터 행렬 W가 행렬 곱 연산에 활용될 것이고, 파라미터 벡터 b가 브로드캐스팅 덧셈 연산에 활용될 것이다.\n",
    "W = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])\n",
    "b = torch.FloatTensor([2, 2])\n",
    "print(W.size()) # torch.Size([3, 2])\n",
    "print(b.size()) # torch.Size([2])\n",
    "# 3 x 2 크기의 행렬 W와 2개의 요소를 갖는 벡터 b를 선언한다.\n",
    "# 이 텐서들을 파라미터로 삼아, 선형 계층 함수를 구성해볼 수 있다.\n",
    "x = torch.FloatTensor(4, 3)\n",
    "print(x.size()) # torch.Size([4, 3])\n",
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    \n",
    "    return y\n",
    "\n",
    "y = linear(x, W, b)\n",
    "print(y.size()) # torch.Size([4, 2]) <- 4 x 3 크기의 행렬이 되어야하나 4 x 2 행렬이 반환된다.\n",
    "# 즉, 파이토치 입장에서 위 방법은 제대로 된 계층(layer)으로 취급되지 않는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 제대로된 계층 만들기\n",
    "# torch.nn.Module 클래스 상속받기\n",
    "# 파이토치에는 nn(neural networks) 패키지가 있고, 내부에는 미리 정의된 많은 신경망들이 있다.\n",
    "# 그리고 그 신경망들은 torch.nn.Module이라는 추상 클래스를 상속받아 정의되어 있다.\n",
    "# 즉, 이 추상 클래스를 상속받아 선형 계층을 구현할 수 있다.\n",
    "\n",
    "import torch.nn as nn # nn 패키지를 불러온다.\n",
    "# nn 패키지를 불러온 후, nn.Module을 상속받을 MyLinear라는 클래스를 정의한다.\n",
    "# nn.Module을 상속받은 클래스는 보통 2개의 메서드(__init__, 과 forward)를 오버라이드한다.\n",
    "# __init__ 함수는 계층 내부에서 필요한 변수를 미리 선언하고 있으며, 심지어 또 다른 계층(nn.Module을 상속받은 클래스의 객체)을 소유할도록 할 수도 있다.\n",
    "# forward 함수는 계층을 통과하는데 필요한 계산 수행을 한다.\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "    \n",
    "    # Should override 'forward' method to implement detail\n",
    "    # The input arguments and outputs can be designed as you wish\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "\n",
    "        return y\n",
    "\n",
    "linear = MyLinear(3, 2)\n",
    "y = linear(x)\n",
    "# 위에서 중요한 점은 forward 함수를 따로 호출하지 않고 객체명에 바로 괄호를 열어 텐서 x를 인수로 넘겨주었다는 것이다.\n",
    "# 이처럼, nn.Module의 상속받는 객체는 __call__함수와 forward가 매핑되어 있어서 forward를 직접 부를 필요가 없다.\n",
    "# forward 호출 앞뒤로 추가적으로 호출하는 함수가 파이토치 내부에 따로 있기 때문에 사용자가 직접 forward 함수를 호출하는 것은 권장되지 않는다.\n",
    "\n",
    "# 하지만, 위의 방ㅅ힉도 제대로 된 방법은 아니다.\n",
    "# 파이토치가 MyLinear라는 클래스의 계층으로 인식하고, 계산도 수행하지만, 내부에 학습할 수 있는 파라미터는 없는것으로 인식한다.\n",
    "# 즉, 아래와 같은 코드를 실행하면 아무것도 출력되지 않는다.\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[3.3983e+21, 1.3591e+22],\n",
      "        [1.6502e-07, 1.6782e-07],\n",
      "        [6.8249e-07, 1.0682e-05]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.0054e-19,  4.5914e-41], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 올바른 방법 nn.Parameter 활용하기\n",
    "# 제대로 된 방법은 W와 b를 파이토치에서 핛브이 가능하도록 인식할 수 있는 파라미터로 만들어야 하는데 torch.nn.Parameter 클래스를 활용하면 된다.\n",
    "# 다음 코디와 같이 파이토치 텐서 선언 이후, nn.Parameter로 감싸준다.\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "\n",
    "        return y\n",
    "\n",
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0633,  0.4604,  0.5159],\n",
      "        [-0.3282, -0.4286,  0.3333]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0931, 0.2235], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear 활용하기\n",
    "# torch.nn에 미리 정의된 선형 계층을 불러다 쓸 수 있다.\n",
    "# 다음 코드는 nn.Linear를 통해 선형 계층을 활용한 모습이다.\n",
    "linear = nn.Linear(3, 2)\n",
    "y = linear(x)\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)\n",
    "'''\n",
    "Parameter containing:\n",
    "tensor([[ 0.0633,  0.4604,  0.5159],\n",
    "        [-0.3282, -0.4286,  0.3333]], requires_grad=True)\n",
    "Parameter containing:\n",
    "tensor([0.0931, 0.2235], requires_grad=True)\n",
    "'''\n",
    "# 또한, 앞서 말한 대로 nn.Module을 상속받아 정의한 나만의 계층 클래스는 내부의 nn.Module 하위 클래스를 소유할 수 있다.\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y\n",
    "\n",
    "# 앞의 코드는 nn.Module을 상속받아 MyLienar 클래스를 정의하고 있는데, __init__ 함수 내부에는 nn.Linear를 선언하여 self.linear에 저장하는 모습을 보여주고 있다.\n",
    "# 또한, forward 함수에서는 self.linear에서 텐서 x를 통과시킨다.\n",
    "# 즉, 이 코드도 선형 계층을 구현한 것이라 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3076e+22, 2.1353e+20],\n",
      "        [1.3076e+22, 2.1353e+20]], device='cuda:0')\n",
      "tensor([[ 1.3076e+22,  2.1353e+20],\n",
      "        [-1.9877e-19,  4.5914e-41]])\n",
      "tensor([[ 1.3076e+22,  2.1353e+20],\n",
      "        [-1.9877e-19,  4.5914e-41]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=2, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용하기\n",
    "# Cuda 함수\n",
    "x = torch.cuda.FloatTensor(2, 2)\n",
    "print(x) # tensor([[0., 0.], [0., 0.]], device='cuda:0')\n",
    "# 텐서 x의 device가 cuda:0으로 잡혀있는걸 볼 수 있따.\n",
    "# cuda: 뒤에 붙은 숫자는 GPU 디바이스의 인덱스를 의미한다.\n",
    "# 즉, 첫 번째 디바이스인 0번 GPU를 의미한다.\n",
    "# 앞의 방법 외에도, 텐서의 cuda 함수를 통해 CPU 메모리 상에 선언된 텐서를 GPU로 복사하는 방법도 존재한다.\n",
    "x = torch.FloatTensor(2, 2)\n",
    "print(x) # tensor([[0.0000e+00, 4.5914e-41],[1.3076e+22, 2.1353e+20]])\n",
    "x = x.cuda()\n",
    "print(x) # tensor([[2.1120e+20, 4.2770e-05],[3.3556e-06, 4.5914e-41]], device='cuda:0')\n",
    "\n",
    "# cuda 함수의 인자에 복사하고자 하는 목적지 GPU 장치의 인덱스를 넣어 원하는 디바이스에 복사할 수도 있다.\n",
    "# x = x.cuda(device=1) # gpu가 1개만 있다면 오류가 발생한다.\n",
    "# print(x)\n",
    "\n",
    "# cuda 함수는 텐서뿐만 아니라 nn.Module의 하위 클래스 객체에도 똑같이 적용할 수 있다.\n",
    "import torch.nn as nn\n",
    "layer = nn.Linear(2, 2)\n",
    "layer.cuda(0)\n",
    "# 주의해야 할 점은, 텐서는 cuda 함수를 통해 원하는 디바이스로 복사가 되지만, nn.Module 하위 클래스 객체의 경우 복사가 아닌 이동(move)이 수행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(x) \u001b[39m# tensor([[0., 0.],[0., 0.]])\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m x \u001b[39m+\u001b[39m x\u001b[39m.\u001b[39mcuda(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# 서로 다른 장치 간 연산\n",
    "# 서로 다른 장치에 올라가 있는 텐서 또는 nn.Module의 하위 클래스 객체끼리는 연산이 불가능하다.\n",
    "# CPU와 GPU에 위치한 텐서들끼리 연산이 불가능할 뿐만 아니라, 0번 GPU와 1번 GPU 사이의 연산도 불가능하다.\n",
    "x = torch.FloatTensor(2, 2)\n",
    "print(x) # tensor([[0., 0.],[0., 0.]])\n",
    "# x + x.cuda(0) # RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# CPU 함수\n",
    "# 필요에 따라 GPU 메모리 상에 있는 텐서를 CPU 메모리로 복사해야 하는 상황이 있을 수 있다.\n",
    "# 이때는 CPU 함수를 사용한다.\n",
    "x = torch.cuda.FloatTensor(2, 2)\n",
    "print(x) # tensor([[0., 0.],[0., 0.]], device='cuda:0')\n",
    "x = x.cpu()\n",
    "print(x) # tensor([[0., 0.],[0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  4.5914e-41],\n",
      "        [-5.8004e-34,  7.3148e-43]])\n",
      "tensor([[ 0.0000e+00,  4.5914e-41],\n",
      "        [-5.8004e-34,  7.3148e-43]], device='cuda:0')\n",
      "tensor([[ 0.0000e+00,  4.5914e-41],\n",
      "        [-5.8004e-34,  7.3148e-43]])\n"
     ]
    }
   ],
   "source": [
    "# To 함수\n",
    "# 파이토치는 원래 cuda 함수와 cpu 함수만 제공했지만, 현재는 to 함수도 함께 제공한다.\n",
    "# to 함수는 원하는 디바이스의 정보를 담은 객체를 인자로 받아, 함수 자신을 호출한 객체를 해당 디바이스로 복사(이동) 시킨다.\n",
    "# 디바이스 정보를 담은 객체는 torch.device를 통해 생성할 수 있다.\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "# 앞서 만든 각 디바이스 객체를 통해 원하는 장치로 복사한다.\n",
    "x = torch.FloatTensor(2, 2)\n",
    "print(x) # tensor([[ 0.0000e+00,  4.5914e-41],[-5.8004e-34,  7.3148e-43]])\n",
    "x = x.to(gpu_device)\n",
    "print(x) # tensor([[ 0.0000e+00,  4.5914e-41],[-5.8004e-34,  7.3148e-43]], device='cuda:0')\n",
    "x = x.to(cpu_device)\n",
    "print(x) # tensor([[ 0.0000e+00,  4.5914e-41],[-5.8004e-34,  7.3148e-43]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device 속성\n",
    "# 텐서는 device 속성을 갖고 있어, 해당 텐서가 위치한 디바이스를 쉽게 파악할 수 있다.\n",
    "x = torch.cuda.FloatTensor(2, 2)\n",
    "x.device # device(type='cuda', index=0)\n",
    "# 흥미로운 점은 nn.Module의 하위 클래스 객체는 해당 속성을 갖고 있지 않다는 점이다.\n",
    "# 즉, 모델이 어느 장치에 올라가 있는지 알고 싶다면 아래와 같은 방법을 사용할 수 있다.\n",
    "layer = nn.Linear(2, 2)\n",
    "next(layer.parameters()).device # device(type='cpu')\n",
    "# parameters 함수를 통해 모델 내의 파라미터에 대한 이터레이터(iterator)를 얻은 후, 첫 번째 파라미터 텐서의 device 속성에 접근한다.\n",
    "# 물론, 이 방법은 모델 내부의 파라미터 전체가 같은 디바이스에 위치해야 한다는 전제가 필요하나, 대부분의 경우 이 전제는 성립한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
