{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[1,2], [3,4]])\n",
    "\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt = torch.LongTensor([[1, 2], [3, 4]])\n",
    "\n",
    "lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 255],\n",
       "        [  0,   0],\n",
       "        [  0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = torch.ByteTensor(3, 2)\n",
    "\n",
    "bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 값으로 채워진 원하는 크기의 텐서를 만들고자 한다면, 다음과 같이 간단하게 만들 수 있다.\n",
    "x = torch.FloatTensor(3, 2)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 넘파이 호환\n",
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4]]) # Define numpy array\n",
    "\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# ndarray(numpy 배열)을 파이토치 텐서로 변환한다.\n",
    "x = torch.from_numpy(x)\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 텐서를 넘파이 ndarray로 변환한다.\n",
    "x = x.numpy()\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 타입 변환\n",
    "# Float 타입 텐서를 Long 타입 텐서로 변환\n",
    "print(ft.long()) # Float -> Long\n",
    "\n",
    "print(lt.float()) # Long -> Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "torch.Size([3, 2, 2])\n",
      "2 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "# 텐서 크기 구하기\n",
    "# 텐서 크기를 구하려면, size() 함수나 shape 속성에 접근한다.\n",
    "# 두 방법의 차이는 없고, size() 함수의 결괏값이 shape 속성에 담겨있다고 보면 된다.\n",
    "x = torch.FloatTensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "# 크기 정보는 배열에담겨있다고 생각할 수 있다.\n",
    "# 즉, 특정 차원의 크기를 알기 위해선, shape 속성의 해당 차원 인덱스에 접근하거나, size() 함수의 인자에 원하는 차원의 인덱스를 넣어주면 된다.\n",
    "print(x.size(1), x.shape[1])\n",
    "\n",
    "# 음수를 넣어주면 뒤에서부터의 순서에 해당한다.\n",
    "print(x.size(-1), x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 텐서 차원의 개수 구하기\n",
    "# 텐서 차원의 개수를 알기 위해선, dim() 함수를 활용한다.\n",
    "# 이것은 shape 속성의 배열 크기와 같다.\n",
    "print(x.dim())\n",
    "print(len(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [6., 7.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 0.,  1.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.0000, 1.3333]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [27., 64.]])\n",
      "tensor([[False,  True],\n",
      "        [ True, False]])\n",
      "tensor([[ True, False],\n",
      "        [False,  True]])\n"
     ]
    }
   ],
   "source": [
    "# 요소별 산술 연산\n",
    "a = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "b = torch.FloatTensor([[2, 2], [3, 3]])\n",
    "\n",
    "# 행렬 사이의 덧셈\n",
    "print(a + b)\n",
    "# 행렬 사이의 뺄셈\n",
    "print(a - b)\n",
    "# 행렬 사이의 곱셈\n",
    "print(a * b)\n",
    "# 행렬 사이의 나눗셈\n",
    "print(a / b)\n",
    "# 행렬 제곱\n",
    "print(a ** b)\n",
    "# 논리 연산\n",
    "print(a == b)\n",
    "print(a != b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "인플레이스 연산\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 인플레이스 연산\n",
    "# 텐서 연산들의 결과 텐서는 메모리에 새롭게 할당된다.\n",
    "# 즉, 빈 메모리의 공간에 결과 텐서가 할당되고 결과 텐서의 값이 위치하게 된다.\n",
    "\n",
    "# 인플레이스(in-place) 연산은 같은 산술 연산을 수행하지만, 기존 텐서에 결과가 저장된다.\n",
    "print(a)\n",
    "print(a.mul(b)) # a.mul(b)의 연산 결과는 새로운 메모리에 할당된다. 즉, 텐서 a를 출력하면, a의 값은 그대로인 것을 볼 수 있다.\n",
    "print(a)\n",
    "\n",
    "# 인플레이스 연산들은 밑줄이 함수명 뒤에 붙어있는 것이 특징이다.\n",
    "# 즉, 곱셈 함수의 인플레이스 연산은 mul_()으로 대응된다.\n",
    "print(\"인플레이스 연산\")\n",
    "print(a.mul_(b))\n",
    "# 즉, 연산 결과는 위와 같지만, 이 곱셈 연산의 결과는 텐서 a에 저장된다.\n",
    "print(a)\n",
    "# 파이토치는 가비지 컬렉터가 효율적으로 동작하기에 굳이 인플레이스 연산을 사용할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(2.5000)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "# 차원 축소 연산 : 합과 평균\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "# sum(), mean() 함수를 통해, 행렬 전체 요소의 합과 평균을 구할 수 있다.\n",
    "# 행렬 요소 전체의 합이나 평균은 텐서나 행렬이 아닌 스칼라 값으로 저장되므로, 차원이 축소된다고 볼 수 있다.\n",
    "print(x.sum())\n",
    "print(x.mean())\n",
    "\n",
    "# 함수의 dim 인자에 원하는 연산의 차원을 넣어줄 수 있다.\n",
    "# dim 인자의 값은 없어지는 차원이라고 생각할 수 있다.\n",
    "print(x.sum(dim=0))\n",
    "# dim=0이면, 첫 번째 차원을 이야기함으로 행렬의 세로축에 대해서 합 연산을 수행한다.\n",
    "# 수식에서 T표시는 행렬의 행과 열을 바꿔 표현하는 전치(transpose) 연산을 의미한다.\n",
    "# dim 인자에 -1을 넣어주면, 뒤에서 첫 번째 차원을 의미한다.\n",
    "# 위에서는 2개의 차원만 존재하므로 dim=-1은 dim=1과 동일한 결과를 반환한다.\n",
    "print(x.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서 + 스칼라\n",
      "tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "torch.Size([2, 2])\n",
      "텐서 + 벡터\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "tensor([[ 4.,  7.],\n",
      "        [ 7., 13.]])\n",
      "텐서 들의 덧셈\n",
      "torch.Size([1, 1, 2])\n",
      "torch.Size([2])\n",
      "tensor([[[4., 7.]]])\n",
      "torch.Size([1, 1, 2])\n",
      "텐서 + 텐서\n",
      "torch.Size([1, 2])\n",
      "torch.Size([2, 1])\n",
      "tensor([[4., 5.],\n",
      "        [6., 7.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 브로드캐스트 연산\n",
    "# 크기가 다른 두 텐서를 갖고 산술 연산을 수행한다.\n",
    "\n",
    "# 텐서 + 스칼라\n",
    "# 행렬(텐서)에 스칼라를 더한다.\n",
    "print(\"텐서 + 스칼라\")\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = 1\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())\n",
    "# 행렬 x의 각 요소에 모두 1이 더해진다.\n",
    "\n",
    "# 텐서 + 벡터\n",
    "# 행렬에 벡터가 더해지는 경우\n",
    "print(\"텐서 + 벡터\")\n",
    "x = torch.FloatTensor([[1, 2,], [4, 8]])\n",
    "y = torch.FloatTensor([3, 5])\n",
    "print(x.size()) # torch.Size([2, 2])\n",
    "print(y.size()) # torch.Size([2])\n",
    "# 크기가 다른 두 텐서 사이의 연산을 위해 브로드케스팅(broadcasting)이 적용된다.\n",
    "# 차원에 맞춰 줄을 세우고 빈칸의 값이 1이라고 가정할 때 다른 한쪽에 똑같이 맞춘다.\n",
    "# [2, 2]    [2, 2]    [2, 2]\n",
    "# [   2] -> [1, 2] -> [2, 2]\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# 텐서들의 덧셈\n",
    "print(\"텐서 들의 덧셈\")\n",
    "x = torch.FloatTensor([[[1, 2]]])\n",
    "y = torch.FloatTensor([3, 5])\n",
    "print(x.size()) # torch.Size([1, 1, 2])\n",
    "print(y.size()) # torch.Size([2])\n",
    "# [1, 1, 2]    [1, 1, 2]\n",
    "# [      2] -> [1, 1, 2]\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())\n",
    "\n",
    "# 텐서 + 텐서\n",
    "# 이 브로드캐스팅규칙은 차원의 크기가 1인 차원에 대해서도 비슷하게 적용된다.\n",
    "# 다음과 같인 두 텐서를 선언하고, 크기를 출력한다.\n",
    "print(\"텐서 + 텐서\")\n",
    "x = torch.FloatTensor([[1, 2]])\n",
    "y = torch.FloatTensor([[3], [5]])\n",
    "print(x.size()) \n",
    "print(y.size())\n",
    "# [1, 2] -> [2, 2]\n",
    "# [2, 1] -> [2, 2]\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11., 12.]]])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11., 12.]]])\n",
      "True\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 형태 변환\n",
    "# 텐서의 전체 요소(element) 개수는 유지한 채 모양을 바꾸는 방법\n",
    "\n",
    "# 1. view 함수\n",
    "x = torch.FloatTensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "print(x.size()) # torch.Size([3, 2, 2])\n",
    "# view 함수의 인자로는 원하는 텐서의 크기를 넣어주면 된다.\n",
    "# 중요한 점은 텐서의 요소 개수는 유지되어야 한다.\n",
    "print(x.view(12)) # tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
    "print(x.view(3, 4))\n",
    "'''\n",
    "tensor([[ 1.,  2.,  3.,  4.],\n",
    "        [ 5.,  6.,  7.,  8.],\n",
    "        [ 9., 10., 11., 12.]])\n",
    "'''\n",
    "print(x.view(3, 1, 4))\n",
    "'''\n",
    "tensor([[[ 1.,  2.,  3.,  4.]],\n",
    "\n",
    "        [[ 5.,  6.,  7.,  8.]],\n",
    "\n",
    "        [[ 9., 10., 11., 12.]]])\n",
    "'''\n",
    "\n",
    "# 새로운 크기가 기존 텐서의 요소 개수와 맞지 않으면 오류가 발생하게 된다.\n",
    "# 하지만, view 함수에 인자를 넣어줄 때 -1을 활용하여 일일이 요소 개수를 맞추기 위해 노력할 필요가 없다.\n",
    "# -1을 활용하면, 일일이 요소 개수를 맞추기 위해 노력할 필요가 없다.\n",
    "# -1이 들어간 차원의 크기는 다른 차원의 값들을 곱하고 남은 필요한 값이 자동으로 채워지게 된다.\n",
    "print(x.view(-1)) # tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
    "print(x.view(3, -1))\n",
    "'''\n",
    "tensor([[ 1.,  2.,  3.,  4.],\n",
    "        [ 5.,  6.,  7.,  8.],\n",
    "        [ 9., 10., 11., 12.]])\n",
    "'''\n",
    "print(x.view(-1, 1, 4))\n",
    "'''\n",
    "tensor([[[ 1.,  2.,  3.,  4.]],\n",
    "\n",
    "        [[ 5.,  6.,  7.,  8.]],\n",
    "\n",
    "        [[ 9., 10., 11., 12.]]])\n",
    "'''\n",
    "# 이때 view 함수의 결과 텐서 주소는 바뀌지 않는다.\n",
    "# 따라서 다음 코드에서 y의 값이 바뀌게 된다면, x의 값도 바뀌게 된다.\n",
    "y = x.view(3, 4)\n",
    "print(x.storage().data_ptr() == y.storage().data_ptr())\n",
    "\n",
    "# view 함수는 메모리에 순차대로 선언된 텐서에 대해서만 작동한다.\n",
    "# 위의 조건에 만족하지 않는다면 오류를 발생시키게 된다.\n",
    "# 위의 오류가 발생한 상황에서 텐서 형태 변환을 진행하려면, contiguous 함수를 호출한 후, view 함수를 호출하면 된다.\n",
    "# contiguous 함수는 텐서를 새로운 메모리상의 인접한 주소에 인접한 값을 순서대로 할당해주는 함수이다.\n",
    "# 이미 메모리상에 원하는 형태로 존재한다면, 새롭게 할당하지 않고 해당 텐서를 contiguous 함수의 결괏값으로 그대로 반환한다.\n",
    "\n",
    "# 2. reshape 함수를 활용할 수 도 있다.\n",
    "# reshape 함수는 view 함수와 동일하게 작동하지만, contiguous 함수와 view 함수를 순차적으로 호출한 것과 동일하다.\n",
    "print(x.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 3. squeeze 함수\n",
    "x = torch.FloatTensor([[[1, 2], [3, 4]]])\n",
    "print(x.size()) # torch.Size([1, 2, 2])\n",
    "# squeeze 함수는 차원의 크기가 1인 차원을 없애주는 역할을 한다.\n",
    "print(x.squeeze()) # tensor([[1., 2.],[3., 4.]])\n",
    "print(x.squeeze().size()) # torch.Size([2, 2])\n",
    "# 텐서 x의 첫 번째 차원의 크기가 1이었기 때문에 squeeze 함수를 통해 텐서 x의 형태는 2 x 2로 바뀌었다.\n",
    "# 다른 함수들과 마찬가지로 squeeze의 경우에도 원하는 차원의 인덱스를 지정할 수 있다.\n",
    "# 만약 해당 차원의 크기가 1이 아닌 경우 같은 텐서가 반환횐다.\n",
    "print(x.squeeze(0).size()) # torch.Size([2, 2])\n",
    "print(x.squeeze(1).size()) # torch.Size([1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 2, 1])\n",
      "torch.Size([2, 2, 1])\n",
      "torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 4. unsqueeze 함수\n",
    "# squeeze 함수의 반대 역할을 한다.\n",
    "# unsqueeze 함수는 지정된 차원의 인덱스에 차원의 크기가 1인 차원을 삽입한다.\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x.size()) # torch.Size([2, 2])\n",
    "\n",
    "# 2 x 2 행렬에 unsqueeze를 수행하여 형태를 변환한다.\n",
    "# 다른 함수들과 마찬가지로 차원의 인덱스에 음수를 넣어 뒤에서부터 접근할 수 있다.\n",
    "print(x.unsqueeze(1).size())  # torch.Size([2, 1, 2])\n",
    "print(x.unsqueeze(-1).size()) # torch.Size([2, 2, 1])\n",
    "print(x.unsqueeze(2).size())  # torch.Size([2, 2, 1])\n",
    "\n",
    "# reshape 함수를 이용하여 똑같이 구현할 수 있다.\n",
    "print(x.reshape(2, 2, -1).size()) # torch.Size([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[ 9., 10.],\n",
      "         [11., 12.]]])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 9., 10.],\n",
      "        [11., 12.]])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 5.,  6.],\n",
      "        [ 9., 10.]])\n",
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 자르기, 붙이기\n",
    "# 하나의 텐서를 둘 이상으로 자르거나, 둘 이상의 텐서를 하나로 합친다.\n",
    "x = torch.FloatTensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "print(x.size()) # torch.Size([3, 2, 2])\n",
    "\n",
    "# x 텐서의 첫 번째 차원의 0번 인덱스만 잘라내고(슬라이싱) 싶다면 아래와 같이 구현한다.\n",
    "print(x[0]) # tensor([[1., 2.],[3., 4.]])\n",
    "# 첫 번째 차원(0번 인덱스)은 잘라내는 과정에서 사라진다.\n",
    "# 즉, 3 x 2 x 2 크기의 텐서를 잘라내어, 2 x 2 크기의 행렬을 얻는다.\n",
    "\n",
    "# 음수를 넣어서 뒤에서부터 접근하는 것도 가능하다.\n",
    "print(x[-1]) # tensor([[ 9., 10.],[11., 12.]])\n",
    "\n",
    "# 첫 번째 차원이 아닌 중간 차원에 대해서 비슷한 작업을 수행하고 싶을 경우에는 콜론(:) 기호를 사용하면 된다.\n",
    "# 콜론을 사용하면, 해당 차원에서는 모든 값을 가져오라는 의미가 된다.\n",
    "print(x[:, 0])\n",
    "'''\n",
    "tensor([[ 1.,  2.],\n",
    "        [ 5.,  6.],\n",
    "        [ 9., 10.]])\n",
    "'''\n",
    "\n",
    "# 첫 번째 차원에서 인덱스 1 이상부터 2 이전까지의 부분을, 두 번째 차원에서는 1 이상부터의 부분을 마지막 차원에서는 전부를 가져왔을 때의 크기를 반환하는 코드이다.\n",
    "print(x[1:2, 1:, :].size()) # torch.Size([1, 1, 2])\n",
    "\n",
    "# 범위를 통해 텐서의 부분을 얻어낼 경우, 차원의 개수가 줄어들지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 2. split 함수\n",
    "# split 함수는 텐서를 특정 차원에 대하여 원하는 크기로 잘라준다.\n",
    "# split 함수를 통해 첫 번째 차원의 크기가 4가 되도록 텐서를 등분한 후, 각각의 등분된 텐서 크기를 출력하는 코드이다.\n",
    "x = torch.FloatTensor(10, 4)\n",
    "splits = x.split(4, dim=0)\n",
    "for s in splits:\n",
    "    print(s.size())\n",
    "'''\n",
    "torch.Size([4, 4])\n",
    "torch.Size([4, 4])\n",
    "torch.Size([2, 4])\n",
    "'''\n",
    "# 주어진 텐서의 첫 번째 차원의 크기가 10이었기 때문에, 크기 4로 등분할 경우 마지막에 크기 2의 텐서가 남게 된다.\n",
    "# 즉, 마지막 텐서의 크기는 다른 텐서들과 달리 2 x 4의 크기가 된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3. chunk 함수\n",
    "# chunk 함수는 크기에 상관없이 원하는 개수로 나눈다.\n",
    "x = torch.FloatTensor(8, 4)\n",
    "# 첫 번째 차원의 크기 8을 최대한 같은 크기로 3등분한다.\n",
    "chunks = x.chunk(3, dim=0)\n",
    "for c in chunks:\n",
    "    print(c.size())\n",
    "'''\n",
    "torch.Size([3, 4])\n",
    "torch.Size([3, 4])\n",
    "torch.Size([2, 4])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "tensor([[[5., 5.],\n",
      "         [6., 6.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [4., 4.]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 4. index select 함수\n",
    "# 특정 차원에서 원하는 인덱스의 값만 취하는 함수이다.\n",
    "# 다음 코드 같이 3 x 2 x 2 크기의 텐서를 만든다.\n",
    "x = torch.FloatTensor([[[1, 1], [2, 2]], [[3, 3], [4, 4]], [[5, 5], [6, 6]]])\n",
    "indice = torch.LongTensor([2, 1])\n",
    "print(x.size()) # torch.Size([3, 2, 2])\n",
    "\n",
    "# indice 텐서의 값을 활용하여, index_select 함수를 수행하면 다음과 같이 진행된다.\n",
    "y = x.index_select(dim=0, index=indice)\n",
    "print(y)\n",
    "'''\n",
    "tensor([[[5., 5.],\n",
    "         [6., 6.]],\n",
    "\n",
    "        [[3., 3.],\n",
    "         [4., 4.]]])\n",
    "'''\n",
    "print(y.size()) # torch.Size([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 3])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n",
      "        [ 4.,  5.,  6., 13., 14., 15.],\n",
      "        [ 7.,  8.,  9., 16., 17., 18.]])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 5. concatenate 함수\n",
    "# 여러 텐서를 합쳐서 하나의 텐서로 만드는 방법이다.\n",
    "# cat 함수의 이름은 Concatenate를 줄여서 부르는 이름이다.\n",
    "# 배열(리스트) 내의 두 개 이상의 텐서를 순서대로 합쳐서 하나의 텐서로 반환한다.\n",
    "# 합쳐지기 위해선, 다른 차원들의 크기가 같아야 한다.\n",
    "x = torch.FloatTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = torch.FloatTensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n",
    "print(x.size(), y.size()) # torch.Size([3, 3]) torch.Size([3, 3])\n",
    "\n",
    "# 3 x 3 텐서 x, y가 있을 때 두 텐서를 원하는 차원으로 이어붙인다.\n",
    "# 아래의 코드는 첫 번째 차원으로 이어붙이는 코드이다.\n",
    "z = torch.cat([x, y], dim=0)\n",
    "print(z)\n",
    "'''\n",
    "tensor([[ 1.,  2.,  3.],\n",
    "        [ 4.,  5.,  6.],\n",
    "        [ 7.,  8.,  9.],\n",
    "        [10., 11., 12.],\n",
    "        [13., 14., 15.],\n",
    "        [16., 17., 18.]])\n",
    "'''\n",
    "print(z.size()) # torch.Size([6, 3])\n",
    "\n",
    "# 마지막 차원(dim=-1)으로 텐서를 이어붙인다.\n",
    "z = torch.cat([x, y], dim=-1)\n",
    "print(z)\n",
    "'''\n",
    "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n",
    "        [ 4.,  5.,  6., 13., 14., 15.],\n",
    "        [ 7.,  8.,  9., 16., 17., 18.]])\n",
    "'''\n",
    "print(z.size()) # torch.Size([3, 6])\n",
    "\n",
    "# 이어 붙이고자 하는 차원의 크기가 맞지 않으면 다음 그림과 같이 cat 함수를 수행할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[ 1., 10.],\n",
      "         [ 2., 11.],\n",
      "         [ 3., 12.]],\n",
      "\n",
      "        [[ 4., 13.],\n",
      "         [ 5., 14.],\n",
      "         [ 6., 15.]],\n",
      "\n",
      "        [[ 7., 16.],\n",
      "         [ 8., 17.],\n",
      "         [ 9., 18.]]])\n",
      "torch.Size([3, 3, 2])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 6. stack 함수\n",
    "# cat 함수와 비슷한 역할을 수행한다.\n",
    "# stack 함수는 쌓기 작업을 수행한다.\n",
    "x = torch.FloatTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = torch.FloatTensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n",
    "z = torch.stack([x, y])\n",
    "print(z)\n",
    "'''\n",
    "tensor([[[ 1.,  2.,  3.],\n",
    "         [ 4.,  5.,  6.],\n",
    "         [ 7.,  8.,  9.]],\n",
    "\n",
    "        [[10., 11., 12.],\n",
    "         [13., 14., 15.],\n",
    "         [16., 17., 18.]]])\n",
    "'''\n",
    "print(z.size()) # torch.Size([2, 3, 3])\n",
    "# 텐서 z의 크기를 출력하여 볼 수 있듯이, 맨 앞에 새로운 차원이 생겨 배열 내의 텐서 개수만큼의 크기가 된 것을 볼 수 있따.\n",
    "# 즉, 새로운 차원을 만든 뒤, 이어 붙이기(cat 함수)를 수행한 것과 같다.\n",
    "\n",
    "# 새롭게 생겨날 차원의 인덱스를 직접 지정할 수도 있다.\n",
    "z = torch.stack([x, y], dim=-1)\n",
    "print(z)\n",
    "'''\n",
    "tensor([[[ 1., 10.],\n",
    "         [ 2., 11.],\n",
    "         [ 3., 12.]],\n",
    "\n",
    "        [[ 4., 13.],\n",
    "         [ 5., 14.],\n",
    "         [ 6., 15.]],\n",
    "\n",
    "        [[ 7., 16.],\n",
    "         [ 8., 17.],\n",
    "         [ 9., 18.]]])\n",
    "'''\n",
    "print(z.size()) # torch.Size([3, 3, 2])\n",
    "\n",
    "# stack 함수는 새로운 차원을 만든 뒤, cat 함수를 수행한 것과 같다.\n",
    "# unsqueeze 함수와 cat 함수를 사용해서 동일하게 구현할 수 있다.\n",
    "d = 0\n",
    "z = torch.cat([x.unsqueeze(d), y.unsqueeze(d)], dim=d)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000e+00, 1.0526e-05],\n",
      "         [4.1773e-03, 7.4409e-43]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[       nan, 2.6848e-06],\n",
      "         [1.1210e-44, 0.0000e+00]],\n",
      "\n",
      "        [[1.0842e-19, 5.2207e-01],\n",
      "         [2.8026e-44, 0.0000e+00]]])\n",
      "torch.Size([5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# cat 함수나 stack 함수는 실전에서 유용하게 사용될 때가 많다.\n",
    "# 특히 여러 이터레이션(iteration)을 돌며 반복되는 작업을 수행한 후 반복 작업의 결과물을 하나로 합치는데 사용된다.\n",
    "# 이 경우에는 주로 다음 코드와 같은 형태를 띄게 된다.\n",
    "result = []\n",
    "for i in range(5):\n",
    "    x = torch.FloatTensor(2, 2)\n",
    "    result += [x]\n",
    "\n",
    "result = torch.stack(result)\n",
    "# result라는 빈 배열(리스트)을 만든 후, 결과물(텐서 x)을 result에 차례대로 추가(append)한 후, stack 또는 cat 함수를 통해 하나의 텐서로 만드는 작업이다.\n",
    "print(result)\n",
    "print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n",
      "tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n",
      "torch.Size([2, 3, 2])\n",
      "tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "# 다양한 기타 함수\n",
    "# 1. expand 함수\n",
    "# expand 함수는 차원의 크기가 1인 차원을 원하는 크기로 늘려주는 함수이다.\n",
    "# 동일한 텐서를 그냥 반복하여, 리스트에 넣고, cat 함수를 해당 차원에 대하여 수행하는 것과 동일하다.\n",
    "x = torch.FloatTensor([[[1, 2]], [[3, 4]]])\n",
    "print(x.size()) # torch.Size([2, 1, 2])\n",
    "# 위의 코드와 같이 두 번째 차원의 크기가 1인 텐서가 존재할 때 다음 코드처럼 expand를 수행할 수 있다.\n",
    "# 여기선, 두 번째 차원의 크기를 3으로 늘린다.\n",
    "y = x.expand(2, 3, 2)\n",
    "print(y)\n",
    "'''\n",
    "tensor([[[1., 2.],\n",
    "         [1., 2.],\n",
    "         [1., 2.]],\n",
    "\n",
    "        [[3., 4.],\n",
    "         [3., 4.],\n",
    "         [3., 4.]]])\n",
    "'''\n",
    "print(y.size()) # torch.Size([2, 3, 2])\n",
    "# 이것을 cat 함수를 통해 구현하면 다음과 같다.\n",
    "y = torch.cat([x] * 3, dim=1)\n",
    "print(y)\n",
    "'''\n",
    "tensor([[[1., 2.],\n",
    "         [1., 2.],\n",
    "         [1., 2.]],\n",
    "\n",
    "        [[3., 4.],\n",
    "         [3., 4.],\n",
    "         [3., 4.]]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 6, 5, 3, 0, 2, 9, 4, 8, 7])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 2. random permutation 함수\n",
    "# randperm 함수는 랜덤 수열을 생성하는 파이토치 함수이다.\n",
    "# 딥러닝은 랜덤성에 의존하는 부분이 많기 때문에 필요에 따라 이 함수를 활용할 수 있다.\n",
    "# randperm 함수의 인자로 숫자를 넣어주면, 1부터 해당 숫자까지의 정수를 임의의 순서로 텐서에 넣어 반환한다.\n",
    "x = torch.randperm(10)\n",
    "print(x) # tensor([1, 6, 5, 3, 0, 2, 9, 4, 8, 7])\n",
    "print(x.size()) # torch.Size([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  5,  9],\n",
      "         [19,  1,  4],\n",
      "         [23,  7, 26]],\n",
      "\n",
      "        [[11, 10,  6],\n",
      "         [22,  8, 24],\n",
      "         [ 2, 12, 20]],\n",
      "\n",
      "        [[25, 13, 17],\n",
      "         [15,  3, 14],\n",
      "         [18, 16, 21]]])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[2, 0, 2],\n",
      "        [0, 2, 2],\n",
      "        [0, 0, 2]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3. Argument Max 함수\n",
    "# argmax 함수는 수식에서도 굉장히 많이 활용된다.\n",
    "# argmax는 함수의 출력값을 최대로 만드는 입력값을 반환하는 함수이다.\n",
    "x = torch.randperm(3 ** 3).reshape(3, 3, -1)\n",
    "print(x)\n",
    "'''\n",
    "tensor([[[ 1, 26, 25],\n",
    "         [18,  3,  4],\n",
    "         [13, 10,  8]],\n",
    "\n",
    "        [[20, 15, 24],\n",
    "         [ 0,  9, 17],\n",
    "         [ 5, 19,  7]],\n",
    "\n",
    "        [[16,  2, 11],\n",
    "         [21, 23, 12],\n",
    "         [ 6, 14, 22]]])\n",
    "'''\n",
    "print(x.size()) # torch.Size([3, 3, 3])\n",
    "# 다음과 같이 argmax 함수를 사용한다.\n",
    "y = x.argmax(dim=-1)\n",
    "print(y)\n",
    "'''\n",
    "tensor([[2, 0, 2],\n",
    "        [0, 2, 2],\n",
    "        [0, 0, 2]])\n",
    "'''\n",
    "print(y.size()) # torch.Size([3, 3])\n",
    "# argmax 함수의 인수로 차원의 인덱스를 -1로 지정했기에, 다른 차원들이 같은 값 중에서 가장 큰 값의 인덱스를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 4. Top-k 함수\n",
    "# argmax 함수의 상우 ㅣ호환 버전이다.\n",
    "# topk 함수는 가장 큰 k 개의 값과 인덱스를 모두 반환한다.\n",
    "# 앞서 선언한 텐서 x에 대하여, topk 함수를 수행한 결과이다.\n",
    "values, indices = torch.topk(x, k=1, dim=-1)\n",
    "print(values.size()) # torch.Size([3, 3, 1])\n",
    "print(indices.size()) # torch.Size([3, 3, 1])\n",
    "\n",
    "# topk 함수는 상위 k개 값과 인덱스를 모두 반환하기에, 반환값을 튜플로 받는 것을 볼 수 있다.\n",
    "# 현재는 k = 1이므로, values와 indices의 마지막 차원의 크기가 1로 되어 있다.\n",
    "# 만일 k를 1보다 더 큰 값을 쓸 경우, 반환되는 텐서의 크기는 다음과 같이 바뀐다.\n",
    "_, indices = torch.topk(x, k=2, dim=-1)\n",
    "print(indices.size()) # torch.Size([3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 2])\n",
      "tensor([[[ 9,  5,  0],\n",
      "         [19,  4,  1],\n",
      "         [26, 23,  7]],\n",
      "\n",
      "        [[11, 10,  6],\n",
      "         [24, 22,  8],\n",
      "         [20, 12,  2]],\n",
      "\n",
      "        [[25, 17, 13],\n",
      "         [15, 14,  3],\n",
      "         [21, 18, 16]]])\n"
     ]
    }
   ],
   "source": [
    "# 5. sort 함수\n",
    "# 앞서 선언한 텐서 x를 원하는 차원을 기준으로 정렬한 후, k개를 추출한다.\n",
    "# 즉, 결과물은 topk 함수와 동일하다.\n",
    "_, indices = torch.topk(x, k=2, dim=-1)\n",
    "print(indices.size()) # torch.Size([3, 3, 2])\n",
    "\n",
    "# topk를 통해 sort 함수를 구현할 수 있다.\n",
    "target_dim = -1\n",
    "values, indices = torch.topk(x,\n",
    "                             k=x.size(target_dim),\n",
    "                             largest=True)\n",
    "\n",
    "print(values)\n",
    "'''\n",
    "tensor([[[ 9,  5,  0],\n",
    "         [19,  4,  1],\n",
    "         [26, 23,  7]],\n",
    "\n",
    "        [[11, 10,  6],\n",
    "         [24, 22,  8],\n",
    "         [20, 12,  2]],\n",
    "\n",
    "        [[25, 17, 13],\n",
    "         [15, 14,  3],\n",
    "         [21, 18, 16]]])\n",
    "'''\n",
    "# 결과물을 보면, 마지막 차원 내에서만 내림차순으로 정렬된 것을 볼 수 있따."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [ True,  True,  True]])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4., -1.],\n",
      "        [-1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 6. Masked Fill 함수\n",
    "# 텐서 내의 원하는 부분만 특정 값으로 바꿔치기한다.\n",
    "# 아래는 3 x 3 크기의 텐서의 내부 값을 0부터 8까지 순서대로 갖도록 하는 코드이다.\n",
    "x = torch.FloatTensor([i for i in range(3**2)]).reshape(3, -1)\n",
    "print(x)\n",
    "'''\n",
    "tensor([[0., 1., 2.],\n",
    "        [3., 4., 5.],\n",
    "        [6., 7., 8.]])\n",
    "'''\n",
    "print(x.size()) # torch.Size([3, 3])\n",
    "\n",
    "# 논리 연산자를 통해, 불리언 텐서를 만든다.\n",
    "mask = x > 4\n",
    "print(mask)\n",
    "'''\n",
    "tensor([[False, False, False],\n",
    "        [False, False,  True],\n",
    "        [ True,  True,  True]])\n",
    "'''\n",
    "# 이 mask를 통해서, masked_fill 함수를 수행한다면, 4보다 큰 값을 갖는 요소들을 특정 값으로 치환할 수 있따.\n",
    "# 아래 코드는 4보다 큰 값들을 모두 -1로 한 번에 치환하도록 하는 코드이다.\n",
    "y = x.masked_fill(mask, value=-1)\n",
    "print(y)\n",
    "'''\n",
    "tensor([[ 0.,  1.,  2.],\n",
    "        [ 3.,  4., -1.],\n",
    "        [-1., -1., -1.]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 7. Ones, Zeros 함수\n",
    "# 아래는 1로 가득찬 2 x 3 텐서와, 0으로 가득 찬 같은 크기의 텐서를 구하는 코드이다.\n",
    "print(torch.ones(2, 3))\n",
    "'''\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]])\n",
    "'''\n",
    "print(torch.zeros(2, 3))\n",
    "'''\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 0., 0.]])\n",
    "'''\n",
    "# 또는 ones_like와 zeros_like 함수를 통해, 특정 텐서와 같은 크기의 0 또는 1 텐서를 만들 수 있다.\n",
    "x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x.size()) # torch.Size([2, 3])\n",
    "print(torch.ones_like(x))\n",
    "'''\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.]])\n",
    "'''\n",
    "print(torch.zeros_like(x))\n",
    "'''\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 0., 0.]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
