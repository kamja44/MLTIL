{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [0, 0, 1, 1]\n",
    "CENTERS = [(-3, -3), (3, 3), (3, -3), (-3, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=2, output_size=3, \n",
    "                 num_hidden_layers=1, hidden_activation=nn.Sigmoid):\n",
    "        \"\"\"가중치 초기화\n",
    "\n",
    "        매개변수:\n",
    "            input_size (int): 입력 크기\n",
    "            hidden_size (int): 은닉층 크기\n",
    "            output_size (int): 출력 크기\n",
    "            num_hidden_layers (int): 은닉층 개수\n",
    "            hidden_activation (torch.nn.*): 활성화 함수\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.module_list = nn.ModuleList()\n",
    "        \n",
    "        interim_input_size = input_size\n",
    "        interim_output_size = hidden_size\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.module_list.append(nn.Linear(interim_input_size, interim_output_size))\n",
    "            self.module_list.append(hidden_activation())\n",
    "            interim_input_size = interim_output_size\n",
    "            \n",
    "        self.fc_final = nn.Linear(interim_input_size, output_size)\n",
    "        \n",
    "        self.last_forward_cache = []\n",
    "       \n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        \"\"\"MLP의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 함수를 위한 플래그\n",
    "                크로스 엔트로피 손실을 사용하려면 반드시 False로 지정해야 합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape는 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        self.last_forward_cache = []\n",
    "        self.last_forward_cache.append(x.to(\"cpu\").numpy())\n",
    "\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "            self.last_forward_cache.append(x.to(\"cpu\").data.numpy())\n",
    "            \n",
    "        output = self.fc_final(x)\n",
    "        self.last_forward_cache.append(output.to(\"cpu\").data.numpy())\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # 한 번에 입력할 샘플 개수\n",
    "input_dim=3\n",
    "hidden_dim=100\n",
    "output_dim=4\n",
    "\n",
    "# 모델 생성\n",
    "mlp=MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤한 입력으로 MLP 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(f\"타입: {format(x.type())}\")\n",
    "    print(f\"크기: {format(x.shape)}\")\n",
    "    print(f\"값: \\n{format(x)}\")\n",
    "\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 분류기로 확률 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_data(batch_size):\n",
    "    assert len(CENTERS) == len(LABELS), \"centers should have equal number labels\"\n",
    "\n",
    "    x_data = []\n",
    "    y_targets = np.zeros(batch_size)\n",
    "    n_centers = len(CENTERS)\n",
    "\n",
    "    for batch_i in range(batch_size):\n",
    "        center_idx = np.random.randint(0, n_centers)\n",
    "        x_data.append(np.random.normal(loc=CENTERS[center_idx]))\n",
    "        y_targets[batch_i]=LABELS[center_idx]\n",
    "    \n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(perceptron, x_data, y_truth, n_samples=1000, ax=None, epoch=None, \n",
    "                      title='', levels=[0.3, 0.4, 0.5], linestyles=['--', '-', '--']):\n",
    "    _, y_pred = perceptron(x_data, apply_softmax=True).max(dim=1)\n",
    "    y_pred = y_pred.data.numpy()\n",
    "\n",
    "    x_data = x_data.data.numpy()\n",
    "    y_truth = y_truth.data.numpy()\n",
    "\n",
    "\n",
    "    n_classes = len(set(LABELS))\n",
    "\n",
    "    all_x = [[] for _ in range(n_classes)]\n",
    "    all_colors = [[] for _ in range(n_classes)]\n",
    "    \n",
    "    colors = ['orange', 'green']\n",
    "    markers = ['o', '*']\n",
    "    edge_color = {'o':'orange', '*':'green'}\n",
    "    \n",
    "    for x_i, y_pred_i, y_true_i in zip(x_data, y_pred, y_truth):\n",
    "        all_x[y_true_i].append(x_i)\n",
    "        if y_pred_i == y_true_i:\n",
    "            all_colors[y_true_i].append('white')\n",
    "        else:\n",
    "            all_colors[y_true_i].append(colors[y_true_i])\n",
    "\n",
    "    all_x = [np.stack(x_list) for x_list in all_x]\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "        \n",
    "    for x_list, color_list, marker in zip(all_x, all_colors, markers):\n",
    "        ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor=edge_color[marker], marker=marker, facecolor=color_list, s=100)\n",
    "    \n",
    "        \n",
    "    xlim = (min([x_list[:,0].min() for x_list in all_x]), \n",
    "            max([x_list[:,0].max() for x_list in all_x]))\n",
    "            \n",
    "    ylim = (min([x_list[:,1].min() for x_list in all_x]), \n",
    "            max([x_list[:,1].max() for x_list in all_x]))\n",
    "            \n",
    "    # 초평면\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        Z = perceptron(torch.tensor(xy, dtype=torch.float32), \n",
    "                       apply_softmax=True)\n",
    "        Z  = Z[:, i].data.numpy().reshape(XX.shape)\n",
    "        ax.contour(XX, YY, Z, colors=colors[i], levels=levels, linestyles=linestyles)\n",
    "    \n",
    "    # 부가 출력\n",
    "    plt.suptitle(title)\n",
    "  \n",
    "    if epoch is not None:\n",
    "        plt.text(xlim[0], ylim[1], \"Epoch = {}\".format(str(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 데이터 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "x_data, y_truth = get_toy_data(batch_size=1000)\n",
    "\n",
    "x_data = x_data.data.numpy()\n",
    "y_truth = y_truth.data.numpy().astype(np.int64)\n",
    "\n",
    "n_classes = len(set(LABELS))\n",
    "\n",
    "all_x = [[] for _ in range(n_classes)]\n",
    "all_colors = [[] for _ in range(n_classes)]\n",
    "\n",
    "colors = ['orange', 'green']\n",
    "markers = ['o', '*']\n",
    "\n",
    "for x_i, y_true_i in zip(x_data, y_truth):\n",
    "    all_x[y_true_i].append(x_i)\n",
    "    all_colors[y_true_i].append(colors[y_true_i])\n",
    "\n",
    "all_x = [np.stack(x_list) for x_list in all_x]\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "for x_list, color, marker in zip(all_x, all_colors, markers):\n",
    "    ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor=color, marker=marker, facecolor=\"white\", s=100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size=len(set(LABELS))\n",
    "num_hidden_layers=0\n",
    "hidden_size=2 # 실제 사용 x\n",
    "\n",
    "seed=24\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mlp1 = MultilayerPerceptron(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "print(mlp1)\n",
    "batch_size=1000\n",
    "\n",
    "x_data_static, y_truth_static= get_toy_data(batch_size)\n",
    "fig, ax=plt.subplots(1, 1, figsize=(10, 5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, ax=ax, title=\"Initial Perceptron State\", levels=[0.5])\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = len(set(LABELS))\n",
    "num_hidden_layers = 1\n",
    "hidden_size = 2\n",
    "\n",
    "seed = 2\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mlp2 = MultilayerPerceptron(input_size=input_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_hidden_layers=num_hidden_layers, \n",
    "                           output_size=output_size)\n",
    "print(mlp2)\n",
    "batch_size = 1000\n",
    "\n",
    "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "visualize_results(mlp2, x_data_static, y_truth_static, \n",
    "                  ax=ax, title='Initial 2-Layer MLP State', levels=[0.5])\n",
    "\n",
    "losses = []\n",
    "batch_size = 10000\n",
    "n_batches = 10\n",
    "max_epochs = 15\n",
    "\n",
    "loss_change = 1.0\n",
    "last_loss = 10.0\n",
    "change_threshold = 1e-5\n",
    "epoch = 0\n",
    "all_imagefiles = []\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(params=mlp2.parameters(), lr=lr)\n",
    "cross_ent_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    terminate_for_loss_change = loss_change < change_threshold    \n",
    "    terminate_for_epochs = epoch > max_epochs\n",
    "    \n",
    "    return terminate_for_epochs\n",
    "\n",
    "while not early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    for _ in range(n_batches):\n",
    "        # 단계 0: 데이터 추출\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    " \n",
    "        # 단계 1: 그레이디언트 초기화\n",
    "        mlp2.zero_grad()\n",
    "        \n",
    "        # 단계 2: 정방향 계산\n",
    "        y_pred = mlp2(x_data).squeeze()\n",
    "        \n",
    "        # 단계 3: 손실 계산\n",
    "        loss = cross_ent_loss(y_pred, y_target.long())\n",
    "\n",
    "        # 단계 4: 역방향 계산\n",
    "        loss.backward()\n",
    "        \n",
    "        # 단계 5: 옵티마이저 단계 수행\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 부가정보\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        loss_change = abs(last_loss - loss_value)\n",
    "        last_loss = loss_value\n",
    "                \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    visualize_results(mlp2, x_data_static, y_truth_static, ax=ax, epoch=epoch, \n",
    "                      title=f\"{loss_value:0.2f}; {loss_change:0.4f}\")\n",
    "    plt.axis('off')\n",
    "    epoch += 1\n",
    "    all_imagefiles.append(f'mlp2_epoch{epoch}_toylearning.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3개 층을 가진 다층 퍼셉트론 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = len(set(LABELS))\n",
    "num_hidden_layers = 2\n",
    "hidden_size = 2\n",
    "\n",
    "seed = 399\n",
    "\n",
    "torch.manual_seed(seed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
