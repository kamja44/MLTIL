{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치로 구현한 퍼셉트론\n",
    "- 임의 개수의 입력을 받아 affine transform을 수행하고, activation function을 적용한 후 출력 하나를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\"퍼셉트론은 하나의 선형 층입니다.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            input_dim (int): 입력 특성의 크기\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        퍼셉트론의 정방향 계산\n",
    "\n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, num_feautres)이다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape는 (batch, )이다.\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0891, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "mse_loss = nn.MSELoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.randn(3, 5)\n",
    "loss = mse_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크로스 엔트로피 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3809, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad = True)\n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64)\n",
    "loss = ce_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이진 크로스 엔트로피 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6616],\n",
      "        [0.0938],\n",
      "        [0.5502],\n",
      "        [0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad = True))\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\n",
    "loss = bce_loss(probabilities, targets)\n",
    "print(probabilities)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론과 이진 분류를 위한 지도 학습 훈련 반복\n",
    "```\n",
    "# 각 에포크는 전체 훈련 데이터를 사용한다.\n",
    "for epoch_i in range(n_epochs):\n",
    "    # 내부 반복은 데이터셋에 있는 배치에 대해 수행된다.\n",
    "    for batch_i in range(n_batches):\n",
    "        # 0단계 : 데이터 가져오기\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "        # 1단계 : 그레디언트 초기화\n",
    "        perceptron.zero_grad()\n",
    "\n",
    "        # 2단계 : 모델의 정방향 계산 수행하기\n",
    "        y_pred = perceptron(x_data, apply_sigmoid = True)\n",
    "        \n",
    "        # 3단계 : 최적하려는 손실 계산하기\n",
    "        loss = bce_loss(y_pred, y_target)\n",
    "\n",
    "        # 4단계 : 손실 신호를 거꾸로 전파하기\n",
    "        loss.backward()\n",
    "\n",
    "        # 5단계 : 옵티마이저로 업데이트하기\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "옐프 리뷰 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv = \"../../Dataset/yelp/raw_train.csv\",\n",
    "    raw_test_dataset_csv = \"../../Dataset/yelp/raw_test.csv\",\n",
    "    proportion_subset_of_train=0.1,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"../../Dataset/yelp/reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 읽기\n",
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=[\"rating\", \"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 클래스 비율이 동일하도록 만든다.\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "\n",
    "review_subset = []\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(args.proportion_subset_of_train * n_total)\n",
    "    review_subset.extend(item_list[:n_subset])\n",
    "\n",
    "review_subset = pd.DataFrame(review_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wing sauce is like water. Pretty much a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Owning a driving range inside the city limits ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       1  I don't know what Dr. Goldberg was like before...\n",
       "2       1  I'm writing this review to give you a heads up...\n",
       "3       1  Wing sauce is like water. Pretty much a lot of...\n",
       "4       1  Owning a driving range inside the city limits ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1    280000\n",
       "2    280000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유 클래스\n",
    "set(review_subset.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 검증 테스트 세트를 만들기 위한 별점을 기준으로 나눈다.\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in review_subset.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "\n",
    "# 분할 데이터를 만든다.\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "\n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total)\n",
    "    n_val = int(args.val_proportion * n_total)\n",
    "    n_test = int(args.test_proportion * n_total)\n",
    "\n",
    "    # 데이터 포인트에 분할 속성을 추가한다.\n",
    "    for item in item_list[:n_train]:\n",
    "        item[\"split\"] = \"train\"\n",
    "\n",
    "    for item in item_list[n_train:n_train + n_val]:\n",
    "        item[\"split\"] = \"val\"\n",
    "\n",
    "    for item in item_list[n_train + n_val : n_train + n_val + n_test]:\n",
    "        item[\"split\"] = \"test\"\n",
    "    \n",
    "    # 최종 리스트에 추가한다.\n",
    "        final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 데이터를 데이터 프레임으로 만든다.\n",
    "final_reviews = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    164640000\n",
       "val       35280000\n",
       "test      35280000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review 전처리\n",
    "def preprocessing_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "final_reviews.review = final_reviews.review.apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews[\"rating\"] = final_reviews.rating.apply({1:\"negative\", 2:\"positive\"}.get)\n",
    "\n",
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 벡터화 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옐프 리뷰 데이터를 위한 파이토치 데이터셋 클래스\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            review_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (ReviewVectorizer): ReviewVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split==\"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split==\"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split==\"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, self.train_size),\n",
    "            \"val\": (self.val_df, self.validation_size),\n",
    "            \"test\": (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "        @classmethod\n",
    "        def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "            \"\"\"\n",
    "            데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만든다.\n",
    "\n",
    "            매개변수:\n",
    "                review_csv (str): 데이터셋의 위치\n",
    "            반환값:\n",
    "                ReviewDataset의 인스턴스\n",
    "            \"\"\"\n",
    "            review_df = pd.read_csv(review_csv)\n",
    "            train_review_df = review_df[review_df.split==\"train\"]\n",
    "            return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "        \n",
    "        @classmethod\n",
    "        def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "            \"\"\"\n",
    "            데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만든다.\n",
    "            캐시된 ReviewVectorizer 객체를 재사용할 때 사용한다.\n",
    "\n",
    "            매개변수:\n",
    "                review_csv (str): 데이터셋의 위치\n",
    "                vecotrizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "            반환값: \n",
    "                ReviewDataset의 인스턴스\n",
    "            \"\"\"\n",
    "            review_df = pd.read_csv(review_csv)\n",
    "            vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "            return cls(review_df, vectorizer)\n",
    "        \n",
    "        @staticmethod\n",
    "        def load_vectorizer_only(vectorizer_filepath):\n",
    "            \"\"\"\n",
    "            파일에서 ReviceVectorizer 객체를 로드하는 정적 메서드\n",
    "\n",
    "            매개변수:\n",
    "                vectorizer_filepath (str): 직렬화된 ReviewVectorizer 객체의 위치\n",
    "            반환값: \n",
    "                ReviewVectorizer의 인스턴스\n",
    "            \"\"\"\n",
    "            with open(vectorizer_filepath) as fp:\n",
    "                return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "            \n",
    "        def save_vectorizer(self, vectorizer_filepath):\n",
    "            \"\"\"\n",
    "            ReviewVectorizer 객체를 json 형태로 디스크에 저장한다.\n",
    "            매개변수 :\n",
    "                vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "            \"\"\"\n",
    "            with open(vectorizer_filepath, \"w\") as fp:\n",
    "                json.dump(self._vectorizer.to_serializable(), fp)\n",
    "        \n",
    "        def get_vectorizer(self):\n",
    "            \"\"\"\n",
    "            벡터 변환 객체를 반환한다.\n",
    "            \"\"\"\n",
    "            return self._vectorizer\n",
    "        \n",
    "        def set_split(self, split=\"train\"):\n",
    "            \"\"\"\n",
    "            데이터프레임에 있는 열을 사용하여 분할 세트를 선택한다.\n",
    "\n",
    "            매개변수: \n",
    "                split (str): \"train\", \"val\", \"test\" 중 하나\n",
    "            \"\"\"\n",
    "            self._traget_split = split\n",
    "            self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.target_size\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            \"\"\"\n",
    "            파이토치 데이터셋의 주요 진입 메서드\n",
    "\n",
    "            매개변수: \n",
    "                index (int): 데이터 포인트의 인덱스\n",
    "            반환값: \n",
    "                데이터 포인트의 특성(x_data)과 레이블(y_target)로 이뤄진 딕셔너리\n",
    "            \"\"\"\n",
    "            row = self._target_df.iloc[index]\n",
    "\n",
    "            review_vector = self._vectorizer.vectorize(row.review)\n",
    "\n",
    "            rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "            return {\n",
    "                \"x_data\": review_vector,\n",
    "                \"y_target\": rating_index\n",
    "            }\n",
    "        \n",
    "        def get_num_batches(self, batch_size):\n",
    "            \"\"\"\n",
    "            배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환한다.\n",
    "\n",
    "            매개변수: \n",
    "                batch_size (int)\n",
    "            반환값\n",
    "                배치 개수\n",
    "            \"\"\"\n",
    "            return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
