{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코퍼스와 레이블 읽기\n",
    "- 한 줄에서 클래스와 텍스트가 탭(\\t)으로 구분된 데이터의 입력을 받는 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, train_fn, valid_fn, batch_size = 64, device = -1, max_vocab = 999999, min_freq = 1, use_eos = False, shuffle = True):\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        # Define field of the input file\n",
    "        # The input file consists of two fields\n",
    "        self.label = data.Field(sequential = False, use_vocab = True, unk_token = None)\n",
    "        self.text = data.Field(use_vocab = True, batch_first = True, include_lengths = False, eos_token=\"<EOS>\" if use_eos else None)\n",
    "\n",
    "        # Those defined two columns will be delimited by TAB\n",
    "        # Thus, we use TabularDataset to load two columns in the input file\n",
    "        # We would have two separate input file: train_fn, valid_fn\n",
    "        # Files consist of two columns: label field and text field\n",
    "        train, valid = data.TabularDataset.splits(path=\"\", train=train_fn, valid=valid_fn, format=\"tsv\", fields=[(\"label\", self.label), (\"text\", self.text)])\n",
    "\n",
    "        # Those loaded dataset would be feeded into each iterator:\n",
    "        # train iterator and valid iterator\n",
    "        # We sort input sentences by length, to group similar lengths\n",
    "        self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid), batch_size = batch_size, device = \"cuda:%d\" %device if device >= 0 else \"cpu\", shuffle = shuffle, sort_key = lambda x: len(x.text), sort_within_batch = True)\n",
    "\n",
    "        # At last, we make a vocabulary for label and text field.\n",
    "        # It is making mapping table between words and indice\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size = max_vocab, min_freq = min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코퍼스 읽기\n",
    "- 한 라인이 텍스트로만 채워져 있을 때를 위한 코드이다.\n",
    "- 주로 언어 모델을 훈련시키는 상황에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_iter \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mBucketIterator(valid, batch_size \u001b[38;5;241m=\u001b[39m batch_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sort_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mtext), sort_within_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mbuild_vocab(train, max_size \u001b[38;5;241m=\u001b[39m max_vocab)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLanguageModelDataset\u001b[39;00m(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, field, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fields[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Dataset'"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, train_fn, valid_fn, batch_size = 64, device=\"cpu\", max_vocab = 99999999, max_length = 255, fix_length = None, use_bos = True, use_eos = True, shuffle = True):\n",
    "        super(DataLoader, self).__init__()\n",
    "        self.text = data.Field(sequential = True, use_vocab = True, batch_first = True, includes_lengths = True, fix_length = fix_length, init_token=\"<BOS>\" if use_bos else None, eos_token = \"<EOS>\" if use_eos else None)\n",
    "\n",
    "        train = LanguageModelDataset(path = train_fn, fields = [(\"text\", self.text)], max_length = max_length)\n",
    "        valid = LanguageModelDataset(path = valid_fn, fields=[(\"text\", self.text)], max_length = max_length)\n",
    "\n",
    "        self.train_iter = data.BucketIterator(train, batch_size = batch_size, device = \"cuda:%d\" %device if device >= 0 else \"cpu\", shuffle = shuffle, sort_key = lambda x: -len(x.text), sort_within_batch = True)\n",
    "        self.valid_iter = data.BucketIterator(valid, batch_size = batch_size, device=\"cuda: %d\" %device if device >= 0 else \"cpu\", shuffle=False, sort_key = lambda x: -len(x.text), sort_within_batch = True)\n",
    "        self.text.build_vocab(train, max_size = max_vocab)\n",
    "\n",
    "class LanguageModelDataset(data.Dataset):\n",
    "    def __init__(self, path, field, max_length=None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [(\"text\", fields[0])]\n",
    "        \n",
    "        examples = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if max_length and max_length < len(line.split()):\n",
    "                    continue\n",
    "                if line != \"\":\n",
    "                    examples.append(data.Example.fromlist([line, fields]))\n",
    "        super(LanguageModelDataset, self).__init__(examples, field, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬 코퍼스 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3059150372.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.train_iter = data.BucketIteration(train, batch_size=batch_size, device=\"cuda:%d\" %device if device >= 0 else \"cpu\", shuffle=shuffle, sort_key=lambda x: len(x.tgt) + (max_length 8 len(x.src)), sort_within_batch = True)\u001b[0m\n\u001b[1;37m                                                                                                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchtext import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, train_fn = None, valid_fn = None, exts = None, batch_size = 64, device = \"cpu\", max_vocab = 99999999, max_length = 255, fix_length = None, use_bos = True, use_eos = True, shuffle = True, dsl = False):\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        self.src = data.Field(sequential = True, use_vocab = True, batch_first = True, includes_lengths = True, fix_length = fix_length, init_token=\"<BOS>\" if dsl else None, eos_token=\"<EOS>\" if dsl else None)\n",
    "        self.tgt = data.Field(sequential = True, use_vocab = True, batch_size = True, includes_lengths = True, fix_length = fix_length, init_token = \"<BOS>\" if use_bos else None, eos_token = \"<EOS>\" if use_eos else None)\n",
    "\n",
    "    if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "        train = TranslationDataset(path = train_fn, exts = exts, fields = [(\"src\", self.src), (\"tgt\", self.tgt)], max_length = max_length)\n",
    "        valid = TranslationDataset(path=valid_fn, exts=exts, fields=[(\"src\", self.src), (\"tgt\", self.tgt)], max_length=max_length)\n",
    "        self.train_iter = data.BucketIteration(train, batch_size=batch_size, device=\"cuda:%d\" %device if device >= 0 else \"cpu\", shuffle=shuffle, sort_key=lambda x: len(x.tgt) + (max_length 8 len(x.src)), sort_within_batch = True)\n",
    "        self.valid_iter = data.BucketIterator(valid, batch_size = batch_size, device=\"cuda: %d\" % device if device >= 0 else \"cpu\", shuffle=False, sort_key = lambda x: len(x.tgt) + (max_length * len(x.src)), sort_within_batch=True)\n",
    "\n",
    "        self.src.build_vocab(train, max_size = max_vocab)\n",
    "        self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "\n",
    "class TranslationDataset(data.Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "    \n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [(\"src\", fields[0]), (\"trg\", fields[1])]\n",
    "        if not path.endswith(\".\"):\n",
    "            path += '.'\n",
    "        \n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "\n",
    "        examples = []\n",
    "        with open(src_path, encoding = \"utf-8\") as src_file, open(trg_path, encoding=\"utf-8\") as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist(\n",
    "                        [src_line, trg_line], fields\n",
    "                    ))\n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
