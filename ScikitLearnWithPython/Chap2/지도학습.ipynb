{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "지도 학습에는 __분류(Classification)__과 __회귀(Regression)__가 있다.\n",
    "- 분류는 미리 정의된, 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.\n",
    "    - 분류에는 딱 두 개의 클래스로 분류하는 이진 분류와 셋 이상의 클래스로 분류하는 다중 분류로 나뉜다.\n",
    "\n",
    "지도 학습에서는 훈련 데이터로 학습한 모델이 훈련 데이터와 특성이 같다면, 처음 보는 새로운 데이터가 주어져도 정확히 예측할 것이라 기대한다.\n",
    "- 모델이 처음 보는 데이터에 대해서, 정확하게 예측할 수 있다면 이를 훈련 세트에서 테스트 세트로 일반화(Generalization) 되었다고 한다.\n",
    "훈련 세트와 테스트 세트가 매우 비슷하다면, 그 모델이 테스트 세트에서도 정확히 예측하리라 기대할 수 있습니다.\n",
    "- 하지만, 매우 복잡한 모델을 만든다면 훈련 세트에서만 정확한 모델이 될 수 있습니다.\n",
    "\n",
    "과대적합(Overfitting)\n",
    "- 가진 정보를 모두 사용해서, 너무 복잡한 모델을 만든다.\n",
    "- 과대적합은 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져서 새로운 데이터에 일반화 되기 어려울 때 일어난다.\n",
    "과소적합(Underfitting)\n",
    "- 너무 간단한 모델이 선택된다.\n",
    "- 데이터의 면면과 다양성을 잡아내지 못하고, 훈련 세트에도 잘 맞지 않는다.\n",
    "\n",
    "모델을 복잡하게 할수록, 훈련 데이터에 대해서는 더 정확히 예측할 수 있다. 단, 너무 복잡해지면, 훈련 세트의 각 데이터 포인트에 너무 민감해져, 새로운 데이터에 잘 일반화되지 못한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dde7263d83a1425"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "\n",
    "# matplotlib를 이용하여 그래프를 그릴 때 한글 깨짐 방지 설정\n",
    "from matplotlib import font_manager, rc\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\" # 사용할 폰트명 경로 삽입\n",
    "font = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc(\"font\", family=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "forge 데이터셋은 2개의 특성을 가진다.\n",
    "- x 축은 첫 번째 특성이다.\n",
    "- y 축은 두 번째 특성이다.\n",
    "- 산점도는 점 하나가 각 데이터 포인트를 나타낸다.\n",
    "- 점의 색과 모양은 데이터 포인트가 속한 클래스를 나타낸다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a38579bb4f781b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 데이터셋을 만든다.\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "# 산점도를 그린다.\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.legend([\"클래스 0\", \"클래스 1\"], loc=4)\n",
    "plt.xlabel(\"첫 번째 특성\")\n",
    "plt.ylabel(\"두 번째 특성\")\n",
    "print(\"X.shape: \", X.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9860c86fe369b5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "X[:,0]과 X[:,1]은 파이썬의 인덱싱 방법을 사용하여, 2차원 배열(행렬) X에서, 데이터를 선택하는 방법을 나타낸다.\n",
    "- X[:,0]는 X의 모든 행에서 첫 번째 열(인덱스 0)의 값을 선택한다.\n",
    "    - 즉, 첫 번째 특성의 모든 값을 가져온다.\n",
    "- X[:,1]는 X의 모든 행에서 두 번째 열(인덱스 1)의 값을 선택한다.\n",
    "    - 즉, 두 번째 특성의 모든 값을 가져온다.\n",
    "이렇게 선택된 값들은 각각 산점도의 x축과 y축의 값으로 사용된다.\n",
    "    - 즉, X[:,0]은 x 축의 값들을, X[:,1]은 y축의 값들을 나타내게 된다.  \n",
    "\n",
    "X.shape는 X의 형태를 나타내는 튜플을 반환한다.\n",
    "이 튜플의 첫 번째 요소는 X의 행의 수(데이터 샘플의 수)이고, 두 번째 요소는 X의 열의 수(특성의 수)다.\n",
    "- 예를 들어, X.shape가 (100,2)를 반환한다면, X가 100개의 데이터를 가지고 있으며, 2개의 특성을 가진 데이터셋임을 의미한다.\n",
    "- shape 정보는 데이터셋의 크기와 복잡성을 이해하는데 도움이 된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236ef90fb47eaf93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "튜플(tuple)은 여러 개의 요소를 담을 수 있는 컬렉션 타입 중 하나이다.\n",
    "리스트와 유사하지만, 튜플은 한 번 생성되면 그 요소를 변경, 추가, 삭제할 수 없다.\n",
    "이러한 특성 덕분에 튜플은 변경되지 않아야 하는 값들을 저장할 때 주로 사용한다.\n",
    "튜플은 괄호()를 사용하여 생성하며, 각 요소는 쉼표(,)로 구분된다.\n",
    "```\n",
    "# 튜플 생성 예\n",
    "Tuple = (1, 2, 3)\n",
    "```\n",
    "위 튜플은 세 개의 요소 1, 2, 3을 가지고 있다.\n",
    "\n",
    "튜플의 요소에 접근할 떄는 인덱스를 사용한다.\n",
    "인덱스는 0부터 시작하며, 대괄호를 사용하여 요소를 선택할 수 있다.\n",
    "예를 들어, Tuple[0]은 튜플의 첫 번째 요소인 1을 반환한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c011597daf4ba57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel(\"특성\")\n",
    "plt.ylabel(\"타깃\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b3b4319ba8a06b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "회귀 알고리즘 설명에는 인위적으로 만든 wave 데이터셋을 사용한다.\n",
    "wave 데이터셋은 입력 특성 하나와 모델링할 타깃 변수(응답)를 가진다.\n",
    "특성을 x 축에 놓고, 회귀의 타깃(출력)을 y 축에 놓는다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b79d6773ea596753"
  },
  {
   "cell_type": "markdown",
   "source": [
    "특성이 적은 데이터셋(저차원 데이터셋)에서 얻은 직관이 특성이 많은 데이터셋(고차원 데이터셋)에서 그대로 유지되지 않을 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131c8a886e74e61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "인위적인 소규모 데이터셋 외에, scikit-learn에 들어 있는 실제 데이터셋도 두 개를 사용한다.\n",
    "하나는 유방암 종양의 임상 데이터를 기록해놓은 위스콘신 유방암 데이터셋이다.\n",
    "- 각 종양은 양성(benign)과 악성(malignant)으로 레이블되어 있고, 조직 데이터를 기반으로 종양이 악성인지를 예측할 수 있도록 학습한다.\n",
    "- scikit-learn에 있는 load_breast_cancer 함수를 사용하여 불러올 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6bffc13cf44c49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys():\\n\", cancer.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da600b4d1e9803ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "scikit-learn에 포함된 데이터셋은 실제 데이터와 데이터셋 관련 정보를 담고 있는 Bunch 객체에 저장되어 있다.\n",
    "Bunch 객체는 파이썬 딕셔너리(Dictionary)와 비슷하지만, 점 표기법을 사용할 수 있다.\n",
    "즉, bunch['key'] 대신, bunch.key를 사용할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc9a140337e3fef1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bunch는 파이썬에서 사용하는 객체 중 하나로, 속성을 가진 딕셔너리를 만들기 위해 사용한다.\n",
    "이는 딕셔너리의 키를 객체의 속성처럼 접근할 수 있게 한다.\n",
    "\n",
    "일반적인 딕셔너리 사용법\n",
    "```\n",
    "dict_obj = {'key': 'value'}\n",
    "print(dict_obj['key'])  # 'value'\n",
    "```  \n",
    "\n",
    "bunch 객체 사용법\n",
    "```\n",
    "bunch_obj = Bunch(key='value')\n",
    "print(bunch_obj.key)  # 'value'\n",
    "```\n",
    "\n",
    "이러한 방식은 코드를 더욱 읽기 쉽고, 관리하기 쉽게 만든다.\n",
    "단, bunch 객체는 파이썬 표준 라이브러리에 포함되어 있지 않으므로, 별도로 설치하거나, 직접 구현해야 한다.\n",
    "\n",
    "클래스를 사용하여, bunch객체 구현하기\n",
    "```\n",
    "class Bunch:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70dd6cacc89598ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bunch 객체를 만들기 위해 클래스를 정의한다.\n",
    "- 클래스의 이름을 Bunch로 지정하고, 생성자 메서드인 __init__을 정의한다.\n",
    "    - 이 메서드는, **kwargs라는 특별한 매개변수를 받는다.\n",
    "\n",
    "**kwargs 매개변수\n",
    "- 키워드 인자(keyword arguments)를 의미한다.\n",
    "- 이는 함수에 임의의 개수의 키워드 인자를 전달할 수 있게 해주는 파이썬의 문법이다.\n",
    "- 예를 들어, Bunch(key=\"value\", other_key=\"other value\")와 같이 사용할 수 있다.\n",
    "\n",
    "self.__dict__.update(kwargs)는 Bunch 객체의 속성 딕셔너리(__dict__)를 kwargs 딕셔너리로 업데이트한다.\n",
    "- 이렇게 하면, kwargs의 각 키-값 쌍이 Bunch 객체의 속성이 된다.\n",
    "- 이제 Bunch 클래스를 사용하여, bunch 객체를 만들 수 있다.\n",
    "```\n",
    "bunch_obj = Bunch(key='value', other_key='other value')\n",
    "```\n",
    "- 위 코드는 Bunch 클래스의 인스턴스를 생성하고, 이 인스턴스를 bunch_obj 변수에 할당한다.\n",
    "- Bunch(key=\"value\", other_key='other value') 부분은 Bunch 클래스의 생성자 메서드를 호출하며, key='value'와 other_key='other value'는 kwargs 딕셔너리를 형성합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66896f36ecd50d22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 위의 Bunch 클래스를 사용하여, 속성을 가진 객체를 만들 수 있다.\n",
    "```\n",
    "bunch_obj = Bunch(key=\"value\", other_key = \"other value\")\n",
    "print(bunch_obj.key)  # 'value'\n",
    "print(bunch_obj.other_key)  # 'other value'\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6413ea6a09b213b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"유방암 데이터의 형태: \", cancer.data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e3ce292bd7ee07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 데이터셋은 569개의 데이터를 갖고 있으며, 30개의 특성을 갖고 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f35a5f77cda356cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"클래스별 샘플 개수: \\n\", {n : v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbeaf0f08e81365b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "np.bincount(cancer.target)\n",
    "- numpy의 bincount 함수를 사용하여, cancer.target의 각 클래스에 속하는 샘플의 개수를 계산한다.\n",
    "- cancer.target은 각 샘플의 클래스 레이블을 나타낸다.\n",
    "\n",
    "{n : v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}\n",
    "- 이 코드는 python의 dictionary comprehension이라는 기능을 사용한다.\n",
    "    - 이는 리스트 comprehension과 비슷한 개념으로, 간결하게 딕셔너리를 생성할 수 있게 해준다.  \n",
    "- zip(cancer.target_names, np.bincount(cancer.tager))\n",
    "    - zip 함수는 여러 개의 iterable한 객체(리스트, 튜플 등)를 인자로 받아, 동일한 인덱스를 가진 요소끼리 튜플로 묶어주는 역할을 한다.\n",
    "    - 여기서는 cancer.target_names와 np.bincount(cancer.target) 두 개의 리스트를 받아, 각각의 클래스 이름과 그에 해당하는 샘플 개수를 묶어준다. \n",
    "- n : v for n, v in ...\n",
    "    - 이 부분은 dictionary compression의 핵심이다. \n",
    "    - for n, v in ... 부분에서 zip 함수를 통해 묶인 튜플에서, 클래스 이름(n)과 샘플 개수(v)를 출력한다.\n",
    "        - 그리고, n : v 부분에서 이를 딕셔너리의 키와 값으로 설정한다.\n",
    "\n",
    "즉, 위 코드는 각 클래스 이름을 키로, 해당 클래스의 샘플의 개수를 값으로 가지는 딕셔너리를 생성한다.\n",
    "이 딕셔너리는 클래스별 샘플 개수를 나타내게 된다.\n",
    "\n",
    "python의 dictionary comprehension은 리스트 컴프리헨션과 비슷한 기능을 가지고 있다.\n",
    "리스트 컴프리헨션은 리스트를 생성하는 방법 중 하나로, 리스트를 생성하는 코드를 한 줄로 간결하게 작성할 수 있다.\n",
    "\n",
    "dictionary comprehension은 딕셔너리를 생성하는 방법 중 하나로, 딕셔너리를 생성하는 코드를 한 줄로 간결하게 작성할 수 있다.\n",
    "for 루프와 if문을 사용하여 딕셔너리를 생성한다.\n",
    "```\n",
    "{key_expression: value_expression for (key, value) in iterable}\n",
    "```\n",
    "- key_expression은 딕셔너리의 키를 생성하는 표현식이다.\n",
    "- value_expression은 딕셔너리의 값을 생성하는 표현식이다.\n",
    "- iterable은 딕셔너리를 생성할 떄 사용할 iterable객체이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "422a6e94e0941d45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"특성 이름:\\n\", cancer.feature_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365b4cce6ce6fa5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "회귀 분석용 실제 데이터셋으로는 보스턴 주택가격 데이터셋을 사용한다.\n",
    "이 데이터셋에는 데이터 포인트 506개와 특성 13개가 있다.\n",
    "\n",
    "보스턴 데이터는 사이킷런에서 삭제되었으므로 스킵"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7b4edb0393912b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델의 복잡도와 일반화 사이의 관계를 입증할 수 있는지 살펴본다.\n",
    "이를 위해, 실제 데이터인 유방암 데이터셋을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "912f0a7e07155ff1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=66\n",
    ")\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# 1에서 10까지 n_neighbors를 적용한다.\n",
    "neighbors_settings = range(1, 11)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bedb0ba3e2431114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # 모델 생성\n",
    "    clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # 훈련 세트 정확도 저장\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # 일반화 정확도 저장\n",
    "    test_accuracy.append(clf.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e14b288e383acf13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"훈련 정확도\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"테스트 정확도\")\n",
    "plt.ylabel(\"정확도\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca99ee92ef9e55b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "최근접 이웃의 수가 하나일 때는 훈련 데이터에 대한 예측이 완벽하다.\n",
    "- 이웃의 수가 적을수록 모델이 복잡하기 때문\n",
    "이웃의 수가 늘어나면, 모델은 단순해지고 훈련 데이터의 정확도는 줄어든다.\n",
    "- 이웃을 하나 사용한 테스트 세트의 정확도는 이웃을 많이 사용했을 때보다 낮다.\n",
    "- 이것은 1-최근접 이웃이 모델을 너무 복잡하게 만든다는 것을 설명해준다.\n",
    "- 반대로, 이웃을 10개 사용했을 때는 모델이 너무 단순해서 정확도는 더 나빠진다.\n",
    "- 정확도가 가장 좋을 떄는 중간 정도인 6개를 사용한 경우이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5081be19404235bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "k-최근접 이웃 회귀\n",
    "wave 데이터셋을 이용하여 이웃이 하나인 최근접 이웃을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dde640309308fdd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "280b445b25ff59a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이웃을 둘 이상 사용하여, 회귀 분석을 할 수 있다.\n",
    "여러 개의 최근접 이웃을 사용할 땐 이웃 간의 평균이 예측된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee791ba4ed87b169"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd1cd210734058"
  },
  {
   "cell_type": "markdown",
   "source": [
    "scikit-learn에서 회귀를 위한 k-최근접 이웃 알고리즘은 KNeighborsRegressor에 구현되어 있습니다.\n",
    "사용법은 KNeighborsClassifier와 비슷하다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8d9b1ae478d9fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "\n",
    "# wave 데이터셋을 훈련 세트와 테스트 세트로 나눈다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# 이웃의 수를 3으로 하여, 모델의 객체를 만든다.\n",
    "reg = KNeighborsRegressor(n_neighbors=3)\n",
    "# 훈련 데이터와 타깃을 사용하여, 모델을 학습시킨다.\n",
    "reg.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "651719420a31ec63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"테스트 세트 예측: \\n\", reg.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11515dc25b5701d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "테스트 세트에 대한 예측을 한다.\n",
    "예측 또한 score 메서드를 사용해서 모델을 평가할 수 있다.\n",
    "- score 메서드는 회귀일 땐, R^2 값을 반환한다.\n",
    "    - R^2은 결정계수라고도 한다.\n",
    "        - 회귀 모델이 예측한 값과, 실제 값 간의 상관관계를 나타내는 지표이다.\n",
    "                - 1에 가까울수록, 모델이 데이터의 분산을 잘 설명하고 있음을 나타낸다.\n",
    "        - 회귀 모델에서 예측의 적합도를 측정한 것으로 보통 0과 1 사이의 값이 된다.\n",
    "        - 1은 예측이 완벽한 경우이고, 0은 훈련 세트의 출력값인 y_train의 평균으로만 예측하는 모델의 경우이다.\n",
    "        - R^2은 음수가 될 수 있따.\n",
    "            - 이때는, 예측과 타깃이 상반된 경향을 가지는 경우이다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1d583fb46732d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"테스트 세트의 R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f394b8d9f7f754b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "{:.2f}.format(reg.score(X_test, y_test))는 R^2 값을 소수점 둘째 자리까지 출력하는 코드입니다.\n",
    "이 코드는 format() 메소드를 사용하여 문자열을 포맷팅하고 있다.\n",
    "{:.2f}는 실수형 값을 소수점 둘째 자리까지 출력하라는 의미이며,\n",
    "format() 메소드는 이 값을 출력할 문자열에 삽입한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abb47cac5119e63b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "KNeighborsRegressor 분석\n",
    "\n",
    "1차원 데이터셋에 대해, 가능한 모든 특성 값을 만들어 예측해볼 수 있다.\n",
    "이를 위해, x 축을 따라, 많은 포인트를 생성하여, 테스트 데이터셋을 만든다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c1ed47ea3ebea0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "# -3과 3 사이에 1,000 개의 데이터 포인트를 만든다.\n",
    "line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # 1, 3, 9 이웃을 사용한 예측을 한다.\n",
    "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    reg.fit(X_train, y_train)\n",
    "    ax.plot(line, reg.predict((line)))\n",
    "    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n",
    "    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n",
    "    \n",
    "    ax.set_title(\n",
    "        \"{} 이웃의 훈련 스코어: {:.2f} 테스트 스코어: {:.2f}\".format(\n",
    "            n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)\n",
    "        )\n",
    "    )\n",
    "    ax.set_xlabel(\"특성\")\n",
    "    ax.set_ylabel(\"타깃\")\n",
    "axes[0].legend([\"모델 예측\", \"훈련 데이터/타깃\", \"테스트 데이터/타깃\"], loc=\"best\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef4603861deba467"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이웃을 하나만 사용할 때는, 훈련 세트의 각 데이터 포인트가 예측에 주는 영향이 너무 커서 예측값이 훈련 데이터를 모두 지나간다.\n",
    "- 이는 매우 불안정한 예측을 만들어낸다.\n",
    "- 이웃을 많이 사용하면, 훈련 데이터에는 잘 안 맞을 수 있지만, 더 안정된 예측을 얻게 된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eb593ddd2d9bf36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "KNeighbors 분류기에 중요한 매개변수는 2개이다.\n",
    "- 데이터 포인트 사이의 거리를 재는 방법과, 이웃의 수이다.\n",
    "- 거리를 재는 방법을 고르는 문제는 기본적으로, 여러 환경에서 잘 동작하는 유클리디안 거리 방식을 사용한다.\n",
    "    - KNeighborsClassifier와 KNeighborsRegressor의 객체를 생성할 때 metric 매개변수를 사용하여 거리 측정 방식을 변경할 수 있다.\n",
    "        - metric 매개변수의 기본값은 민코프스키 거리를 의미하는 'minkowski'이며, 거듭제곱의 크기를 정하는 매개변수인 p가 기본값이 2일 때 유클리디안 거리와 같다.\n",
    "\n",
    "\n",
    "K-NN 알고리즘은 모델을 매우 빠르게 만들 수 있지만, 훈련 세트가 매우 크면(특성의 수나 샘플의 수가 클 경우) 예측이 느려진다.\n",
    "- K-NN 알고리즘을 사용할 땐, 데이터를 전처리하는 과정이 매우 중요하다.\n",
    "- 많은 특성을 가진(수백 개 이상) 데이터 셋에는 잘 동작하지 않는다.\n",
    "- 특성 값 대부분이 0인(즉, 희소한) 데이터셋과는 잘 작동하지 않는다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9fd02a5f4c9d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "__선형 모델(Linear model)__\n",
    "- 입력 특성에 대한 선형 함수를 만들어서 예측을 수행한다.\n",
    "\n",
    "1차원 wave 데이터셋으로 파라미터 w[0]와 b를 직선처럼 되도록 학습시킨다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9b1609764b910e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd59b200a6c7c28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "회귀를 위한 선형 모델은 특성이 하나일 떈 직선, 두 개일 땐 평면이 되며, 더 높은 차원(특성이 더 많음)에서는 초평면(hyperplane)이 되는 회귀 모델의 특징을 갖고 있다.\n",
    "\n",
    "특성이 많은 데이터셋이라면, 선형 모델은 매우 훌륭한 성능을 낼 수 있다.\n",
    "- 특히, 훈련 데이터보다 특성이 더 많은 경우엔 어떤 타깃 y도 완벽하게(훈련 세트에 대해서) 선형 함수로 모델링할 수 있다.\n",
    "\n",
    "회귀를 위한 선형 모델은 다양하다.\n",
    "- 이 모델들은 훈련 데이터로부터 모델 파라미터 w와 b를 학습하는 방법과, 모델의 복잡도를 제어하는 방법에서 차이가 난다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c7583c9d1c019b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "선형 회귀(최소제곱법)\n",
    "- 선형 회귀(linear regression) 또는 최소제곱법(OLS: ordinary least squares)은 가장 간단하고, 오래된 회귀용 선형 알고맂므이다.\n",
    "- 선형 회귀는 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차(mean squared error)를 최소화하는 파라미터 w와 b를 찾는다.\n",
    "    - 평균제곱오차는 예측값과 타깃값의 차이를 제곱하여, 더한 후 샘플의 개수로 나눈 것이다. \n",
    "- 선형 회귀는 매개변수가 없는 것이 장점이지만, 매개변수가 없기에 모델의 복잡도를 제어할 방법이 없다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b398d7f3078a2fba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 선형 모델을 만드는 코드이다.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c40008ec7581d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "기울기 파라미터(w)는 가중치(Weight) 또는 계수(Confficient)라고 하며, lr 객체의 coef_ 속성에 저장되어 있다.\n",
    "편향(offset) 또는 절편(intercept) 파라미터(b)는 intercept)_ 속성에 저장되어 있따."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b68c2c1fd95d7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"lr.coef_: \", lr.coef_)\n",
    "print(\"lr.intercept_: \", lr.intercept_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a675ab8482d5f7af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "scikit_learn은 훈련 데이터에서 유도된 속성은 항상 끝에 밑줄을 붙인다.\n",
    "- 사용자가 지정한 매개변수와 구분하기 위함이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9241179dd4d4378"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"훈련 세트 점수 : {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"테스트 세트 점수 : {:.2f}\".format(lr.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb159b524e932c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "훈련 세트 점수와 테스트 세트 점수가 매우 비슷한 것을 알 수 있다.\n",
    "- 이는 모델이 과대적합이 아닌 __과소적합__인 상태를 의미한다. \n",
    "\n",
    "훈련 세트 점수와 테스트 세트 점수에 비해 월등히 높다(훈련 세트 > 테스트 세트)\n",
    "- 이는 모델이 __과대적합__인 상태를 의미한다.\n",
    "    - 즉, 복잡도를 제어할 수 있는 모델을 사용해야 한다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663bb7a77227f3d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "릿지 회귀(Ridge) [L2 규제]\n",
    "- 회귀를 위한 선형 모델이다.\n",
    "- 릿지 회귀에서 가중치(w) 선택은, 훈련 데이터를 잘 예측하기 위해서 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적이 있다.\n",
    "    - 릿지 회귀는 `가중치의 절댓값을 가능한 한 작게 만드는 것이 목적이다.`\n",
    "        -  즉, 가중치(w)_의 모든 원소가 0에 가깝게 되길 원한다.\n",
    "        -  직관적으로 생각하면, 이는 `모든 특성이 출력에 주는 영향을 최소한으로 만든다.(기울기를 작게 만든다.)`\n",
    "            -  이러한 제약을 규제(Regularization)라고 한다.\n",
    "                    - 규제란, 과대적합이 되지 않도록, 모델을 강제로 제한하는 것을 의미한다.\n",
    "                    - 릿지 회귀에서 사용하는 규제 방식을 L2 규제라고 한다.   \n",
    "    - 사용자는 alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지를 지정할 수 있다.\n",
    "        - alpha 매개변수의 기본값은 1.0이다.\n",
    "        - alpha 값을 높이면, 계수를 0에 더 가깝게 만들어서 훈련 세트의 성능은 나빠지지만, 일반화에 도움을 줄 수 있다.\n",
    "            - 즉, alpha 값이 높을 수록, 제약이 더 많은 모델이므로, 작은 alpha 값일 때보다 coef_의 절댓값의 크기가 작을 것이라고 예상할 수 있다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b1d3f3684535e6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.plots.plot_ridge_n_samples()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8720c3c67b069ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "규제의 효과를 이해하는 또 다른 방법은 alpha값을 고정하고, 훈련 데이터의 크기를 변화시켜 보는 것이다.\n",
    "- 데이터셋의 크기에 따른 모델의 성능 변화를 나타낸 그래프를 학습 곡선(learning curve)라고 한다.\n",
    "\n",
    "릿지와 선형회귀 모두, 훈련 세트의 점수가 테스트 세트의 점수보다 높다.\n",
    "- 릿지에는 규제가 적용되므로, 릿지의 훈련 데이터 점수가 전체적으로 선형 회귀의 훈련 데이터 점수보다 낮다.\n",
    "    - 단, 테스트 데이터에서는 릿지의 점수가 더 높으며, 특별히 작은 데이터셋에서는 더 그렇다.\n",
    "        - 데이터셋 크기가 400 미만에서는 선형 회귀는 어떤 것도 학습하지 못하고 있다.\n",
    "- 두 모델의 성능은 데이터가 많아질수록 좋아지고, 마지막에는 선형 회귀가 릿지 회귀를 따라잡는다.\n",
    "    - 데이터를 충분히 주면 규제 항은 덜 중요해져서 릿지 회귀와 선형 회귀의 성능이 같아질 것이라고 기대할 수 있다.\n",
    "    - 또한, 선형 회귀의 훈련 데이터 성능이 감소한다.\n",
    "        - 이는 데이터가 많아질수록 모델이 데이터를 기억하거나 과대적합하기 어려워지기 때문이다.    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acd6a2c12ae8cd95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "라쏘 회귀(Lasso) [L1 규제]\n",
    "- 선형 회귀에 규제를 적용하는 Ridge의 대안이다.\n",
    "- L1 규제라고 한다.\n",
    "- L1 규제의 결과로 라쏘를 사용할 때, 어떤 계수는 정말로 0이 된다.\n",
    "    - 즉, 모델에서 완전히 제외되는 특성이 생긴다.\n",
    "        - 특성 선택(feature selection)이 자동으로 이뤄진다고 볼 수 있다.\n",
    "- 계수를 얼마나 강하게 0으로 보낼지 조절하는 alpha 매개변수를 지원한다. \n",
    "    - alpha 값을 낮추면, 모델의 복잡도는 증가하여, 훈련 세트와 테스트 세트에서의 성능이 좋아진다.\n",
    "    - 그러나, alpha 값을 너무 낮추면 규제의 효과가 없어져 과대적합 되므로, 선형 회귀와 겨로가가 비슷해진다.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea075c17b851f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "릿지 회귀와 라쏘 회귀 중 릿지 회귀를 대체로 선호한다.\n",
    "- 라쏘 회귀는 어떤 계수가 0이 될 수 있기 때문이다.\n",
    "    - 즉, 모델에서 완전히 제외되는 특성이 생길 수 있기 때문이다.\n",
    "    - 단, 특성이 많고, 그중 일부분만 중요하다면 라쏘 회귀가 더 좋은 선택일 수 있다.\n",
    "    - 또한, 라쏘 회귀가 입력 특성 중 일부만 사용하므로, 쉽게 해석할 수 있는 모델을 만들 수 있다.\n",
    "\n",
    "ElasticNet\n",
    "- 라쏘와 릿지의 패널티를 결합한 규제이다.\n",
    "- L1 규제와 L2 규제를 위한 2개의 매개변수 조정이 필요하다.    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1847bc2202b0d562"
  },
  {
   "cell_type": "markdown",
   "source": [
    "선형 분류 알고리즘\n",
    "- 가장 널리 알려진 두 개의 선형 분류 알고리즘은 linear_model.LogisticRegression에 구현된 로지스틱 회귀(Logistic Regression)와 svm.LinearSVC에 구현된 서포트 벡터 머신(support vector machine)이다.\n",
    "- LogisticRegression\n",
    "    - 이름에 Regression(회귀)가 들어가지만, 회귀 알고리즘이 아니라 분류 알고리즘이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "641da7f8c027885"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(max_iter=5000), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"특성 0\")\n",
    "    ax.set_ylabel(\"특성 1\")\n",
    "\n",
    "axes[0].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eac0b035e4358e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "forge 데이터셋을 사용하여, LogisticRegression과 LinearSVC 모델을 만들고, 이 선형 모델들이 만들어낸  결정 경계를 그림으로 나타낸다.\n",
    "\n",
    "forge 데이터셋의 첫 번째 특성을 x 축에 놓고, 두 번째 특성을 y 축에 놓는다.\n",
    "LinearSVC와 LogisticRegression으로 만든 결정 경계가 직선으로 표현되었고, 위쪽은 클래스 1, 아래쪾은 클래스 0으로 나누고 있다.\n",
    "- 즉, 새로운 데이터가 직선 위쪽에 놓이면 클래스 1로 분류되고, 직선 아래쪽에 놓이면 클래스 0으로 분류된다.\n",
    "\n",
    "LogisticRegression과 LinearSVC 모델은 기본적으로 L2 규제를 사용한다.\n",
    "- LogisticRegression과 LinearSVC에서 규제의 강도를 결정한느 매개변수는 C이다.\n",
    "    - C값이 높아지면 규제가 감소한다. \n",
    "    - 즉, 매개변수로 높은 C값을 지정하면 LogisticRegression과 LinearSVC는 훈련 세트에 가능한 최대로 맞추려 한다.\n",
    "    - 매개변수로 낮은 C값을 지정하면, LogisticRegression과 LinearSVC는 계수 벡터(w)가 0에 가까워지도록 만든다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3682cc19f814e4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "매개변수 C의 작동 방식을 다르게 설명할 수 도 있따.\n",
    "알고리즘은 C의 값이 낮아지면, 데이터 포인트 중 다수에 맞추려고 하는 반면, C값을 높이면 개개의 데이터 포인트를 정확히 분류하려고 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34f431f9ddf39596"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LinearSVC를 사용한 예\n",
    "mglearn.plots.plot_linear_svc_regularization()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d7925de99af3e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "왼쪽 그림은 아주 작은 C값 때문에 규제가 많이 적용되었다.\n",
    "규제가 강해진 모델은 비교적 수평에 가까운 결정 경계를 만들었고, 잘못 분류한 데이터는 2개이다.\n",
    "중간 그림은 C값이 조금 더 크며, 잘못 분류한 데이터 포인트는 2개의 샘플에 민감해져, 결정경계가 기울어졌따.\n",
    "오른쪽 그림에서 C값을 아주 크게 하였더니, 결정 경계는 더 기울었고, 마침내 클래스 0의 모든 데이터 포인트를 올바르게 분류했다.\n",
    "- 모든 포인트를 직선으로 완벽히 분류할 수 없기에, 클래스 1의 포인트 하나는 여전히 잘못 분류되어 있다.\n",
    "- 즉, 오른쪽 그림의 모델은 모든 데이터 포인트를 정확히 분류하려고 했지만, 클래스의 전체적인 배치를 잘 파악하지 못한것이다.\n",
    "    - 즉, 오른쪽 모델은 과대적합 되었다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0a1ac4bc948dcad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "회귀와 비슷하게 분류에서의 선형 모델은 낮은 차원의 데이터에서는 결정 경계가 직선이거나 평면이여서 매우 제한적인 것처럼 보인다.\n",
    "- 하지만, 고차원에서는 분류에 대한 선형 모델이 매우 강력해지며, 특성이 많아지면 과대적합되지 않도록 하는 것이 매우 중요해진다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8c7ef17fc5524e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "logreg = LogisticRegression(max_iter=5000).fit(X_train, y_train)\n",
    "print(\"훈련 세트 점수: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"테스트 세트 점수: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a84d5d9b5e2c666"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e421eddef27bf5ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "stratify=cancer.target\n",
    "- 분할 전용 데이터셋에서 각 클래스의 비율을 유지하도록 하는 역할을 한다.\n",
    "- 즉, 각 클래스의 비율이 원본 데이터셋에서도 유지되도록 할 수 있다.\n",
    "\n",
    "기본값 C=1이 훈련 세트와 테스트 세트 양쪽에 95% 정확도의 성능을 내고 있다.\n",
    "- 훈련 세트와 테스트 세트의 성능이 매우 비슷하므로, 과소적합을 의심할 수 있다.\n",
    "    - 과소적합을 해소하기 위해 모델의 제약을 푼다.\n",
    "        - 즉, C값을 증가시킨다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b699baebb59b180d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100, max_iter=5000).fit(X_train, y_train)\n",
    "print(\"훈련 세트 점수: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"테스트 세트 점수: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7a8615fa807384"
  },
  {
   "cell_type": "markdown",
   "source": [
    "C = 100을 사용하니, 훈련 세트의 정확도가 높아졌고, 테스트 세트의 정확도도 조금 증가했따.\n",
    "- 이는 복잡도가 높은 모델일수록 성능이 좋음을 의미한다.\n",
    "\n",
    "규제를 더 강하게 하기 위해 C=0.01을 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30e6e4dc67334534"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01, max_iter=5000).fit(X_train, y_train)\n",
    "print(\"훈련 세트 점수: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"테스트 세트 점수: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5707f9e7f9f8eaeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이미 과소적합된 모델에서, 규제를 더 강하게 했으므로, 훈련 세트와 테스트 세트의 정확도는 기본 매개변수일 때보다 낮아진다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332c67ed3f8b11e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"특성\")\n",
    "plt.ylabel(\"계수 크기\")\n",
    "plt.legend()\n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc6f3e06f14183"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LogisticRegression은 기본적으로 L2 규제를 적용하므로, Ridge로 만든 모습과 비슷하다.\n",
    "- 규제를 강하게 할수록 계수들을 0에 더 가깝게 만들지만, 완전히 0이 되지는 않는다.\n",
    "\n",
    "mean perimeter\n",
    "- 위 글미의 3번째 계수\n",
    "- C=100, C=1,일 때 이 계수는 음수이지만, C=0.001일 때는 양수가 되며, C=1일 때보다도 절댓값이 더 큰것을 알 수 있다.\n",
    "- 이와 같은 모델을 해석하면, 계수가 클래스와 특성의 연관성을 알려줄 수 있다.\n",
    "    - 예를 들어, 높은 \"texture error\" 특성은 악성인 샘플과 관련이 깊다.\n",
    "    - 하지만, mean perimeter 계수의 부호가 바뀌는 것으로 보아, 높은 mean perimeter 값은 양성이나 악성의 신호 모두가 될 수 있다.\n",
    "        - 이는 선형 모델의 계수를 항상 조심해서 해석해야 하는 이유이다.     "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af239c106b082488"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# L1 규제를 사용하여, 위의 해석을 더 쉽게 그래프로 그린다.\n",
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(solver='liblinear', C=C, penalty=\"l1\", max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"C={:.3f} 인 l1 로지스틱 회귀의 훈련 정확도: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_train, y_train)))\n",
    "    print(\"C={:.3f} 인 l1 로지스틱 회귀의 테스트 정확도: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_test, y_test)))\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.xlabel(\"특성\")\n",
    "plt.ylabel(\"계수 크기\")\n",
    "\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)\n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70308f5243f37f46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이진 분류에서의 선형 모델과 회귀에서의 선형 모델 사이에는 유사점이 많다.\n",
    "- 회귀에서처럼, 모델들의 주요 차이점은 규제에서 모든 특성을 이용할지, 일부 특성만 사용할지 결정하는 penalty 매개변수이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9ad39c862df1ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "다중 클래스 분류용 선형 모델\n",
    "- 많은 선형 분류 모델은 이진 분류만을 지원한다.\n",
    "    - 즉, 다중 클래스(multi class)를 지원하지 않는다.    \n",
    "    - 로지스틱 회귀 제외\n",
    "        - 로지스틱 회귀는 소프트맥스(softmax) 함수를 사용한, 다중 클래스 분류 알고리즘을 지원한다.\n",
    "- 이진 분류 알고리즘을 다중 클래스 분류 알고리즘으로 확장하는 보편적인 기법은 일대다 방법이다.\n",
    "    - 각 클래스를 다른 모든 클래스와 구분하도록 이진 분류 모델을 학습시킨다.   \n",
    "        - 즉, 클래스의 수만큼 이진 분류 모델이 만들어진다.\n",
    "        - 예측을 할 때 이렇게 만들어진 모든 이진 분류기가 작동하여, 가장 높은 점수를 내는 분류기의 클래스를 예측값으로 선택한다.\n",
    "        - 클래스별 이진 분류기를 만들면, 각 클래스가 계수 벡터(w)와 절편(b)을 하나씩 갖게 된다.\n",
    "            - 결국, 분류 신뢰도를 나타내는 공식의 결괏값이 가장 높은 클래스가 해당 데이터의 클래스 레이블로 할당된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "795ddac10dc7a049"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3개의 클래스를 가진 간단한 데이터셋에 일대다 방식을 적용한다.\n",
    "- 이 데이터셋은 2차원이며, 각 클래스의 데이터는 정규분포(가우시안 분포)를 따른다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51e8cdc18e22ea5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:,1] ,y)\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")\n",
    "plt.legend([\"클래스 0\", \"클래스 1\", \"클래스 2\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5804d8af4be85f3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 데이터셋으로 LinearSVC 분류기를 훈련한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d08650830a0bbd11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"계수 배열의 크기: \", linear_svm.coef_.shape)\n",
    "print(\"절편 배열의 크기: \", linear_svm.intercept_.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f2b7102fd0077a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "coef_ 배열의 크기는 (3, 2)이다.\n",
    "- coef_의 행은 세 개의 클래스에 각각 대응하는 계수 벡터를 담고 있으며, 열은 각 특성에 따른 계수 값(이 데이터셋에는 2개)을 가지고 있다.\n",
    "intercept_는 각 클래스의 절편을 담은 1차원 벡터이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a3285176428c15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3개의 이진 분류기가 만드는 경계를 시각화한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e53de7859b2b0e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:,0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, mglearn.cm3.colors):\n",
    "        plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")\n",
    "plt.legend(['클래스 0', '클래스 1', '클래스 2', '클래스 0 경계', '클래스 1 경계',\n",
    "            '클래스 2 경계'], loc=(1.01, 0.3))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66f001de45dc703"
  },
  {
   "cell_type": "markdown",
   "source": [
    "휸련 데이터의 클래스 0에 속한 모든 포인트는 클래스 0을 구분하는 직선 위에 있다.\n",
    "- 즉, 이진 분류기가 만든 클래스 0 지역에 있다.\n",
    "- 그러나, 클래스 0에 속한 포인트는 클래스 2를 구분하는 직선 위 즉, 클래스 2의 이진 분류기에 의해 나머지로 분류된다.\n",
    "- 또한, 클래스 0에 속한 포인트는 클래스 1을 구분하는 직선 왼쪽 즉, 클래스 1의 이진 분류기에 의해서도 나머지로 분류된다.\n",
    "    - 즉, 이 영역의 어떤 포인트든 최종 분류기는 클래스 0으로 분류한다.\n",
    "        - 클래스 0 분류 신뢰도 공식의 결과는 0보다 크고, 다른 두 클래스의 경우는 0보다 작을 것이다.  \n",
    "\n",
    "중앙의 삼각형은 분류 공식의 결과가 가장 높은 클래스이다.\n",
    "- 세 분류기가 모두 나머지로 분류한다.\n",
    "- 즉, 가장 가까운 직선의 클래스가 된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e93dc22f45457405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2차원 평면의 모든 포인트에 대한 예측 결과\n",
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['클래스 0', '클래스 1', '클래스 2', '클래스 0 경계', '클래스 1 경계',\n",
    "            '클래스 2 경계'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"특성 0\")\n",
    "plt.ylabel(\"특성 1\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84df01361756e578"
  },
  {
   "cell_type": "markdown",
   "source": [
    "선형 모델의 주요 매개변수\n",
    "- 회귀 모델에서는 alpha이다.\n",
    "- LinearSVC와 LogisticRegression에서는 C이다.\n",
    "- alpha 값이 클수록, C값이 작을수록 모델이 단순해진다. => 규제를 완화한다.\n",
    "- 보통 C와 alpha는 로그 스케일로 최적치를 정한다.\n",
    "    - 로그 스케일은 보통 자릿수가 바뀌도록 10배씩 변경한다.\n",
    "        - 즉, 0.01, 0.1, 1, 10, 100 등 이다.\n",
    "- 최적치를 정한 후, L1 규제를 사용할지, L2 규제를 사용할지 정한다.\n",
    "    - 중요한 특성이 많지 않다고 생각된다면, L1 규제를 사용한다.\n",
    "        - L1 규제는 모델의 해석이 중요한 요송리 때도 사용할 수 있다.\n",
    "            - L1 규제는 몇 가지 특성만 사용하므로, 해당 모델에 중요한 특성이 무엇이고, 그 효과가 어느 정도인지 설명하기 쉽다.\n",
    "    - 그렇지 않다면 기본적으로 L2 규제를 사용한다.      \n",
    "\n",
    "선형 모델은 학습 속도가 빠르고 예측도 빠르다.\n",
    "- 매우 큰 데이터셋과 희소한 데이터셋에도 잘 작동한다.\n",
    "    - 대용량의 데이터셋이라면, 기본 설정보다 빨리 처리하도록 LogisticRegression과 Ridge에 solver='sag' 옵션을 준다.\n",
    "        - sga는 Stohastic Average Gradient descent(확률적 평균 경사 하강법)의 약자이다.\n",
    "            - 경사 하강법과 비슷하지만, 반복이 진행될 때 이전에 구한 모든 경사의 평균을 사용하여 계수를 갱신한다.\n",
    "    - 다른 대안으로서는 선형 모델의 대용량 처리 버전으로 구현된 SGDClassifier와 SGDRegressor를 사용할 수 있다.\n",
    "- 선형 모델은 샘플에 비해 특성이 많들 때 잘 작동한다.\n",
    "    - 다른 모델로 학습하기 어려운 매우 큰 데이터셋에도 선형 모델을 많이 사용한다.\n",
    "        - 그러나, 저차원의 데이터셋에는 다른 모델들의 일반화 성능이 더 좋다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96eb8df953fd4e96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "나이브 베이즈 분류기(naive bayes)\n",
    "- LogisticRegression이나 LinearSVC 같은 선형 분류기보다 훈련 속도가 빠른 편이지만, 그 대신 일반화 성능이 조금 낮다.\n",
    "- 나이브 베이즈 분류기는 각 특성을 개별로 취급하여 파라미터를 학습하고, 각 특성에서 클래스별 통계를 단순 취합한다.\n",
    "- scikit-learn에 구현된 나이브 베이즈 분류기는 GaussianNB, BernoulliNB, MultinomalNB 3가지로 나뉜다.\n",
    "    - GaussianNB\n",
    "        - 연속적인 어떤 데이터에도 적용할 수 있다.\n",
    "        - 클래스별로 각 특성의 표준편차와 평균을 저장한다.\n",
    "    - BernoulliNB\n",
    "        - 이진 데이터에 적용할 수 있다.\n",
    "            - 각 클래스의 특성 중 0이 아닌 것이 몇 개인지 센다.\n",
    "    - MultinomialNB\n",
    "        - 카운트 데이터(특성이 어떤 것을 헤아린 정수 카운트로, 예를 들면 문장에 나타난 단어의 횟수)에 적용할 수 있다.\n",
    "        - 클래스벼로 특성의 평균을 계산한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4299beb47ce90c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BernoulliNB 예\n",
    "X = np.array(\n",
    "    [\n",
    "        [0, 1, 0, 1],\n",
    "        [1, 0, 1, 1],\n",
    "        [0, 0, 0, 1],\n",
    "        [1, 0, 1, 0]\n",
    "    ]\n",
    ")\n",
    "y = np.array([0, 1, 0, 1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0a05b5f0a73486b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이진 특성을 4개 가진 데이터 포인트가 4개 있습니다.\n",
    "클래스는 0과 1 2개이다.\n",
    "출력 y의 클래스가 0인 경우(첫 번쨰와 세 번째 데이터 포인트)\n",
    "- 첫 번째 특성은 0이 두 번이고, 0이 아닌 것은 한 번도 없다.\n",
    "- 두 번째 특성은 0이 한 번이고, 1도 한번이다.\n",
    "- 같은 방식으로, 두 번째 클래스에 해당하는 데이터 포인트에 대해서도 계산한다.\n",
    "    - 클래스별로 0이 아닌 원소를 세는 과정을 요약하면 아래와 같다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "785599ade4d948e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # 각 클래스에 대해 반복\n",
    "    # 특성마다 1 이 나타난 횟수를 센다.\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"특성 카운트:\\n\", counts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8354f4350a499d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "MultinomialNB와 BernoulliNB는 모데의 복잡도를 조정하는 alpha 매개변수 하나를 가지고 있다.\n",
    "- alpha가 주어지면, 알고리즘이 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 alpha 개수만큼 추가한다.\n",
    "    - 이는 통계 데이터를 완만하게 만들어준다.\n",
    "    - alpha가 크면 더 완만해지고, 모델의 복잡도는 낮아진다.\n",
    "    - alpha에 따른 알고리즘 성능 변동은 비교적 크기 않아서, alpha 값이 성능 향상에 크게 기여하지 않는다.\n",
    "        - 그러나 이 값을 조정하면 어느정도는 정확도를 높일 수 있다.\n",
    "\n",
    "나이브 베이즈 모델과 선형 모델의 장단점은 비슷하다.\n",
    "- 훈련과 예측 속도가 빠르며, 훈련 과정을 이해하기 쉽다.\n",
    "- 희소한 고차원 데이터에서 잘 작동하며, 비교적 매개변수에 민감하지 않다.\n",
    "    - 선형 모델로 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에는 나이브 베이즈 모델을 시도해볼 수 있다.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1d302713e0268be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "결정 트리(Decision tree)\n",
    "- 분류와 회귀 문제에 널리 사용하는 모델이다.\n",
    "- 결정 트리는 결저에 다다르기 위해 예/아니오 질문을 이어 나가면서 학습한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4349db8633e6d55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 연속된 질문들은 아래와 같이 결정트리로 나타낼 수 있다.\n",
    "mglearn.plots.plot_animal_tree()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3d2f26e7b73fbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import graphviz\n",
    "print(graphviz.__file__)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa92cdf5c987b978"
  },
  {
   "cell_type": "markdown",
   "source": [
    "트리의 노드는 질문이나 정답을 담은 네모 상자이다.\n",
    "- 리프는 마지막 노드이다.\n",
    "- 에지(edge)는 질문의 답과 다음 질문을 연결한다.\n",
    "\n",
    "결정 트리를 학습한다는 것은 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 뜻이다.\n",
    "- 머신 러닝에서는 이런 질문들을 테스트라고 한다.\n",
    "    - 보통 데이터는 예/아니오 형태의 특성으로 구분되지 않고, 2차원 데이터셋과 같이 연속된 특성으로 구분된다.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d61327a5519ec1df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mglearn.plots.plot_tree_progressive()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead1933347c3db6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "트리를 만들 때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다.\n",
    "\n",
    "반복된 프로세스는 각 노드가 테스트 하나씩을 가진 이진 결정 트리를 만든다.\n",
    "- 다르게 말하면, 각 테스트는 하나의 축을 따라 데이터를 둘로 나누는 것으로 생각할 수 있다.\n",
    "    - 이는 계층적으로 영역을 분할해가는 알고리즘이라고 할 수 있다.\n",
    "    - 각 테스트는 하나의 특성에 대해서만 이뤄지므로, 나눠진 영역은 항상 축에 평행하다.\n",
    "\n",
    "데이터를 분할하는 것은 각 분할된 영역이(결정 트리의 리프) 한 개으 타깃값(하나의 클래스나 하나의 회귀 분석 결과)을 가질 때까지 반복된다.\n",
    "- 타깃 하나로만 이뤄진 리프 노드를 순수 노드(pure node)라고 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9b610bcb7631bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "결정 트리의 복잡도 제어하기\n",
    "- 트리를 만들 때 리프 노드가 순수 노드가 될 때까지 진행하면, 모델이 매우 복잡해지고 훈련 데이터에 과대적합된다.\n",
    "    - 순수 노드로 이뤄진 트리는 훈련 세트에 100% 정확하게 맞는다는 의미이다.\n",
    "    - 즉, 훈련 세트의 모든 데이터 포인트는 정확한 클래스의 리프 노드에 있습니다.\n",
    "- 결정 트리를 만들 때 과대적합을 막는 전략은 크게 2가지가 있다.\n",
    "    - 트리 생성을 일찍 중단하는 전략(사전 가지치기)\n",
    "        - 사전 가지치기 방법은 트리의 최대 깊이나 리프의 최대 개수를 제한하거나, 또는 노드가 분할하기 위한 포인트의 최소 개수를 지정한다.\n",
    "    - 트리를 만든 후, 데이터 포인트가 적은 노드를 삭제하거나 병합하는 전략(사후 가지치기 or 가지치기)이다.\n",
    "\n",
    "scikit-learn에서 결정 트리는 DecisionTreeRegressor와 DecisionTreeClassifier에 구현되어 있다.\n",
    "- scikit-learn은 사전 가지치기만 지원한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b676432e060626"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"훈련 세트 정확도 : {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도 : {:.3f}\".format(tree.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f6f14e7128a98c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "모든 리프 노드가 순수 노드이므로, 훈련 세트의 정확ㄷ는 100%이다.\n",
    "- 즉, 트리는 훈련 데이터의 모든 레이블을 완벽하게 기억할 만큼 충분히 깊게 만들어졌다.\n",
    "- 테스트 세트의 정확도는 이전에 본 선형 모델에서의 정확도인 95%보다 조금 낮다.\n",
    "\n",
    "결정 트리의 깊이를 제한하지 않으면, 트리는 무한정 깊어지고 복잡해질 수 있다.\n",
    "- 그래서 가지치기하지 않은 트리는 과대적합되기 쉽고 새로운 데이터에 잘 일반화되지 않는다.\n",
    "    - 사전 가지치기를 트리에 적용해서, 훈련 데이터에 완전히 학습되기 전에 트리의 성장을 막아 해결할 수 있다.\n",
    "        - 일정 깊이에 도달하면 트리의 성장을 멈추게 할 수 있다.\n",
    "            - max_depth=4 옵션을 주면, 연속된 질문을 4개로 제한한다.\n",
    "            - 트리 깊이를 제한하면, 과대적합이 줄어든다.\n",
    "                - 이는 훈련 세트의 정확도를 떨어뜨리지만, 테스트 세트의 성능은 개선시킨다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e1cd33cc306b3c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도 : {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"테스트 세트 정확도 : {:.3f}\".format(tree.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e8489e3b450f3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "트리 모듈의 export_graphviz 함수를 이용하여 트리를 시각화할 수 있다.\n",
    "- 이 함수는 그래프 저장용 텍스트 파일 포맷인 .dot 파일을 만든다.\n",
    "- 각 노드에서 다수인 클래스를 색으로 나타내기 위해 옵션을 주고, 적절히 레이블되도록 클래스 이름과 특성 이름을 매개변수로 전달한다.\n",
    "    - export_graphviz 함수에 filled 매개변수를 True로 지정하면, 노드의 클래스가 구분되도록 색으로 칠해진다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "817f57ea06ffa499"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"악성\", \"양성\"],\n",
    "                feature_names=cancer.feature_names, impurity=False, filled=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9889ff782b6abd8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이 파일을 읽어서, graphviz 모듈을 사용하여, 시각화할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ef9fe5e2e45c839"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "with open(\"tree.dot\", encoding=\"utf-8\") as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396cef249d23c09d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "트리의 각 노드에 적힌 samples는 각 노드에 있는 샘플의 수를 나타내며, value는 클래스당 샘플의 수를 제공한다.\n",
    "\n",
    "트리의 특성 중요도\n",
    "- 전체 트리를 살펴보는 것은 어려울 수 있으니, 트리가 어떻게 동작하는지 요약하는 속성들을 사용할 수 있다.\n",
    "- 가장 널리 사용되는 속성은 트리를 만드는 결정에 각 특성이 얼마나 중요한지 평가하는 특성 중요도(feature importance)이다.\n",
    "    - 이 값은 0과 1 사이의 숫자로, 각 특성에 대해\n",
    "        - 0은 전혀 사용되지 않았다는 뜻이다.\n",
    "        - 1은 완벽하게 타깃 클래스를 예측했다는 뜻이다.\n",
    "- 특성 중요도의 전체 합은 1이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d96a9c7d88e9865"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"특성 중요도:\\n\", tree.feature_importances_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "991f3a1ddb569ece"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 특성 중요도 시각화\n",
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"특성 중요도\")\n",
    "    plt.ylabel(\"특성\")\n",
    "    plt.ylim(-1, n_features)\n",
    "plot_feature_importances_cancer(tree)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198e4a3f26f1ebbc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫번째 노드에서 사용한 특성(\"worst radius\")이 가장 중요한 특성으로 나타난다.\n",
    "- 이 그래프는 첫 번째 노드에서 두 클래스를 꽤 잘 나누고 있다는 관찰을 뒷받침한다.\n",
    "- 그러나 어떤 특성의 feature_importance_값이 낮다고 해서 이 특성이 유용하지 않다는 뜻은 아니다.\n",
    "    - 단지, 트리가 그 특성을 선택하지 않았을 뿐이며, 다른 특성이 동일한 정보를 지니고 있어서 일 수 있다.\n",
    "- 선형 모델의 계수와는 달리, 특성 중요도는 항상 양수이며, 특성이 어떤 클래스를 지지하는지는 알 수 없다.\n",
    "- 즉, 특성 중요도의 값은 \"worst radius\"가 중요하다고 알려주지만, 높은 반지름이 양성을 의미하는지 악성을 의미하는지는 알 수 없다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c9b76fb859f04a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y축의 특성이 클래스 레이블과 복합적인 관계를 갖고 있는 2차원 데이터셋과 결정 트리가 만든 결정 경계\n",
    "tree = mglearn.plots.plot_tree_not_monotone()\n",
    "display(tree)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bb38d5aed63762"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 그림은 두 개의 특성과 클래스를 가진 데이터셋을 보여준다.\n",
    "- X[1]에 있는 정보만 사용되었고, X[0]은 전혀 사용되지 않았다.\n",
    "- 하지만, X[1]과 출력 클래스와의 관계는 단순하게 비례 또는 반비례하지 않는다.\n",
    "    - 즉, X[1] 값이 높으면 클래스 0이고 값이 낮으면 1이라고(또는 그 반대로) 말할 수 없다.\n",
    "\n",
    "회귀 트리의 사용법과 분석법은 분류 트리와 매우 비슷하다.\n",
    "- 회귀를 위한 트리 기반의 모델을 사용할 때 짚고 넘어가야할 특별한 속성이 있다.\n",
    "    - DecisionTreeRegressor(그리고 모든 다른 트리 기반 회귀 모델)는 외삽(extrapolation) 즉, 훈련 데이터의 범위 밖의 포인트에 대해 예측을 할 수 없다.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "541f0476aa3fdff1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 컴퓨터 메모리 가격 동향 데이터셋\n",
    "# x 축은 날짜, y 축은 해당 년도의 램(RAM) 1메가바이트당 가격이다.\n",
    "import os\n",
    "ram_prices = pd.read_csv(os.path.join(mglearn.datasets.DATA_PATH, \"ram_price.csv\"))\n",
    "plt.yticks(fontname=\"Arial\")\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"년\")\n",
    "plt.ylabel(\"가격 ($/Mbyte)\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6974a94372c5da3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be04d6525a5ecf5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
