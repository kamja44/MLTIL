{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "print(iris_label)\n",
    "print(iris.target_names)\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "iris_df[\"label\"] = iris.target\n",
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 / 테스트용 데이터 분리\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)\n",
    "'''\n",
    "X_train\n",
    "- 학습용 Feature Data Set\n",
    "X_test\n",
    "- 테스트용 Feature Data Set\n",
    "y_train\n",
    "- 학습용 Label Data Set\n",
    "y_test\n",
    "- 테스트용 Label Data Set\n",
    "\n",
    "test_size\n",
    "- 전체 중 테스트 데이터 세트의 비율\n",
    "\n",
    "random_state\n",
    "- 랜덤 seed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "# 학습 = fit\n",
    "dt_clf.fit(X_train, y_train)\n",
    "# 테스트 = predict\n",
    "pred = dt_clf.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 성능 평가\n",
    "# 정확도 = accuracyt_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(정답 레이블 데이터 세트, 예측 레이블 데이터 세트)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test) # 정답\n",
    "print(pred)   # 예측\n",
    "# 전체 30개 데이터 중 28개 일치 -> 0.93333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예제 데이터\n",
    "- sklearn.datasets\n",
    "    - 사이킷런에 내장되어 예제로 제공하는 데이터 세트\n",
    "\n",
    "피처 처리\n",
    "- sklearn.preprocessing\n",
    "    - 데이터 전처리에 필요한 다양한 가공 기능을 제공한다.\n",
    "    - 문자열을 숫자형 코드 값으로 인코딩, 정규화, 스케일링 등\n",
    "- sklearn.feature_selection\n",
    "    - 알고리즘에 큰 영향을 미치는 피처를 우선순위대로 선택 작업을 수행하는 다양한 기능을 제공한다.\n",
    "- sklearn.feature_extraction\n",
    "    - 텍스트 데이터나 이미지 데이터의 벡터화된 피처를 추출한다.\n",
    "\n",
    "피처 처리 & 차원 축소\n",
    "- sklearn.decomposition\n",
    "    - 차원 축소와 관련된 알고리즘을 지원하는 모듈이다.\n",
    "\n",
    "데이터 분리, 검증 및 파라미터 튜닝\n",
    "- sklearn.model_selection\n",
    "    - 교차 검증을 위한 학습용 / 테스트용 분리, 최적 파라미터 추출 등의 API를 제공한다.\n",
    "\n",
    "평가\n",
    "- sklearn.metrics\n",
    "    - 다양한 성능 측정 방법을 제공한다.\n",
    "\n",
    "머신러닝 알고리즘\n",
    "- sklearn.ensemble\n",
    "    - 앙상블 알고리즘 제공\n",
    "- sklearn.linear_model\n",
    "    - 회귀 관련 알고리즘 등을 지원한다.\n",
    "- sklearn.naive_bayes\n",
    "    - 나이브 베이즈 알고리즘을 제공한다.\n",
    "- sklearn.neighbors\n",
    "    - 최근접 이웃 알고리즘을 제공한다.\n",
    "- sklearn.svm\n",
    "    - 서포트 벡터 머신 알고리즘을 제공한다.\n",
    "- sklearn.tree\n",
    "    - 의사 결정 트리 알고리즘을 제공한다.\n",
    "- sklearn.cluster\n",
    "    - 클러스터링 알고리즘을 제공한다.\n",
    "\n",
    "유틸리티\n",
    "- sklearn.pipeline\n",
    "    - 피처 처리 등의 변환과 머신 러닝 알고리즘 학습, 예측 등을 함께 묶어서 실행할 수 있는 유틸리티를 제공한다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "내장된 예제 데이터 세트\n",
    "- 회귀 용도\n",
    "    - datasets.load_boston()\n",
    "        - 미국 보스턴 집 값 데이터 세트\n",
    "    - datasets.load_diabetes()\n",
    "        - 당뇨 데이터 세트\n",
    "- 분류 용도\n",
    "    - datsets.load_breast_cancer()\n",
    "        - 위스콘신 유방암 데이터 세트\n",
    "    - datasets.load_digits()\n",
    "        - 0 ~ 9 숫자 이미지 데이터 세트\n",
    "    - dataset.load_iris()\n",
    "        - 붓꽃 데이터 세트\n",
    "\n",
    "다운로드 예제 데이터 세트\n",
    "- 인터넷에서 내려 받아 scikit_learn_data 폴더에 저장한다.\n",
    "    - fetch_covtype()\n",
    "        - 토지 조사 자료\n",
    "    - fetch_20newsgroups()\n",
    "        - 뉴스 그룹 텍스트 자료\n",
    "    - fetch_olivetti_faces()\n",
    "        - 얼굴 이미지 자료\n",
    "    - fetch_lfw_people()\n",
    "        - 얼굴 이미지 자료\n",
    "    - fetch_lfw_pairs()\n",
    "        - 얼굴 이미지 자료\n",
    "    - fetch_rcv1()\n",
    "        - 로이터 뉴스 말뭉치\n",
    "    - fetch_mldata()\n",
    "        - ML 웹사이트에서 다운로드\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "표본 데이터 생성기\n",
    "- 분류와 클러스터링을 위한 표본 데이터 생성기\n",
    "    - datasets.make_classifications()\n",
    "        - 분류를 위한 데이터 세트를 무작위로 생성한다.\n",
    "    - datasets.make_blobs()\n",
    "        - 클러스터링을 위한 데이터 세트를 무작위로 생성한다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예제 데이터의 구성\n",
    "- 딕셔너리 형태로 저장된다.\n",
    "\n",
    "키(Key)\n",
    "- data\n",
    "    - 피처의 데이터 세트(ndarray)\n",
    "- target\n",
    "    - 분류는 레이블 값, 회귀는 숫자 결과 값(ndarray)\n",
    "- target_names\n",
    "    - 레이블 이름(ndarray or list)\n",
    "- feature_names\n",
    "    - 피처 이름(ndarray or list)\n",
    "- DESCR\n",
    "    - 데이터 세트 및 각 피처에 대한 설명(string)\n",
    "'''\n",
    "# 키 종류\n",
    "print(iris.keys())\n",
    "# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Selection 모듈 소개\n",
    "- 학습 / 테스트 데이터 세트 분리, 교차 검증 분할 및 평가, 하이퍼 파라미터 튜닝을 위한 다양한 함수와 클래스를 제공한다.\n",
    "- 학습 / 테스트 데이터 세트 분리(train_test_split)\n",
    "    - 파라미터들\n",
    "        - test_size\n",
    "            - 전체에서 테스트 데이터 세트의 비율(기본 값 = 0.25)\n",
    "        - train_size\n",
    "            - 전체에서 학습용 데이터 세트의 비율\n",
    "            - 일반적으로 train_size 보다는 test_size를 지정한다.\n",
    "        - shuffle\n",
    "            - 데이터를 분리하기 전에 데이터를 미리 섞을지 결정한다.\n",
    "            - 기본값은 True이다.\n",
    "        - random_state\n",
    "            - 랜덤 seed 역할이다.\n",
    "            - 생략하면 무작위 데이터로 분리한다.\n",
    "            - 일정한 값으로 지정하면, 매번 같은 데이터 세트로 분리한다.\n",
    "    - 반환 값\n",
    "        - 학습용 피처 데이터 세트\n",
    "        - 테스트용 피처 데이터 세트\n",
    "        - 학습용 레이블 데이터 세트\n",
    "        - 테스트용 레이블 데이터 세트 순서의 튜플\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=121)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "교차 검증\n",
    "- 과적합(Overfitting)되지 않도록 다양한 학습과 평가를 수행한다.\n",
    "- 데이터의 편중을 막기 위해 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트로 학습과 평가를 수행한다.\n",
    "- 데이터 세트\n",
    "    - 학습 / 검증 / 테스트 데이터 세트로 분리한다.\n",
    "\n",
    "|-------training--------------------------|-------test-------|\n",
    "|-------training-------|----validation----|-------test-------|\n",
    "실질적으로 test데이터는 학습이 모두 종료된 후 사용한다.\n",
    "- 학습이 제데로 되었는지 확인하기 위해 validation 데이터셋을 사용한다.\n",
    "\n",
    "K 폴드 교차 검증\n",
    "- K개의 데이터 폴드 세트\n",
    "- K = 5인 경우\n",
    "- 첫 번째 학습 / 검증      |학습|학습|학습|학습|검증| -> 검증 평가 1\n",
    "- 두 번째 학습 / 검증      |학습|학습|학습|검증|학습| -> 검증 평가 2\n",
    "- 셋 번째 학습 / 검증      |학습|학습|검증|학습|학습| -> 검증 평가 3\n",
    "- 네 번째 학습 / 검증      |학습|검증|학습|학습|학습| -> 검증 평가 4\n",
    "- 다섯 번째 학습 / 검증    |검증|학습|학습|학습|학습| -> 검증 평가 5\n",
    "                                                    - 교차 검증 최종 평가 => 평균(검증 평가 1 ~ 5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "kflod = KFold(n_splits=5)\n",
    "cv_accuracy= []\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kflod.split(iris_data):\n",
    "    X_train, X_test = iris_data[train_index], iris_data[test_index]\n",
    "    y_train, y_test = iris_label[train_index], iris_label[test_index]\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print(n_iter, accuracy, train_size, test_size)\n",
    "    print(n_iter, test_index)\n",
    "    cv_accuracy.append(accuracy)\n",
    "print(np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "교차 검증\n",
    "- Stratified K 폴드\n",
    "- 불균형한 레이블 분포\n",
    "- 특정 레이블이 특이하게 많거나, 적어서 분포가 치우쳐진다.\n",
    "- 전체 레이블 분포대로 각 폴드의 레이블 분포를 유지한다.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data = iris.data, columns= iris.feature_names)\n",
    "iris_df[\"label\"] = iris.target\n",
    "iris_df[\"label\"].value_counts()\n",
    "\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "kflod = KFold(n_splits=3)\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "cv_accuracy= []\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kflod.split(iris_data):\n",
    "    X_train, X_test = iris_data[train_index], iris_data[test_index]\n",
    "    y_train, y_test = iris_label[train_index], iris_label[test_index]\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print(n_iter, accuracy, train_size, test_size)\n",
    "    print(n_iter, test_index)\n",
    "    cv_accuracy.append(accuracy)\n",
    "print(np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# StratifiedKFold => 원래 데이터의 비율을 유지하며 데이터 세트를 나눈다.\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(iris_df, iris_df[\"label\"]):\n",
    "    n_iter += 1\n",
    "    label_train = iris_df[\"label\"].iloc[train_index]\n",
    "    label_test = iris_df[\"label\"].iloc[test_index]\n",
    "    print(\"iteration\", n_iter)\n",
    "    print(label_train.value_counts())\n",
    "    print(label_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cross_val_score\n",
    "- 교차 검증을 보다 간편하게 수행할 수 있다.\n",
    "- cross_val_score(\n",
    "    estimator,          => Classifier 또는 Regression\n",
    "    X,                  => 피처 데이터 세트\n",
    "    y=None,             => 레이블 데이터 세트\n",
    "    scoring=None,       => 예측 성능 평가 지표\n",
    "    cv=None,            => 교차 검증 폴드 수\n",
    "    n_jobs=1,           \n",
    "    verbose=0,\n",
    "    fit_params=None,\n",
    "    pre_dispatch=`2*n_jobs`\n",
    ")\n",
    "- 성능 지표(scoring) 측정 값을 배열로 반환한다.\n",
    "- 분류의 경우 Stratified K 폴드 방식을 사용하는게 좋다.\n",
    "- 회귀의 경우 K 폴드 방식을 사용하는게 좋다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "scores = cross_val_score(dt_clf, iris_data, iris_label, scoring=\"accuracy\", cv=3)\n",
    "print(np.round(scores, 4))\n",
    "print(np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "교차 검증\n",
    "- 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에 수행한다.\n",
    "    - GridSearchCV\n",
    "        - 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 내는 파라미터의 조합을 찾는다.\n",
    "        - grid_parameters = {\"max_depth\":[1, 2, 3], \"min_samples_split\":[2, 3]}\n",
    "    - GridSearchCV(\n",
    "        estimator,              # classifier, regressor 등\n",
    "        param_grid,             # key + 리스트 값의 딕셔너리, 튜닝을 위한 파라미터 이름과 값\n",
    "        scoring,                # 성능 측정 평가 방법, accuracy 등\n",
    "        cv,                     # 학습 / 테스트 세트의 개수\n",
    "        refit,                  # 최적의 하이퍼 파라미터를 찾은 후, 해당 하이퍼 파라미터로 estimator를 재학습시킨다. 기본값은 True이다.\n",
    "    )\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state=121)\n",
    "dtree = DecisionTreeClassifier()\n",
    "parameters = {\"max_depth\" : [1, 2, 3], \"min_samples_split\" : [2, 3]}\n",
    "\n",
    "grid_dtree = GridSearchCV(dtree, param_grid = parameters, cv=3, refit=True)\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "scores_df[[\"params\", \"mean_test_score\", \"rank_test_score\", \"split0_test_score\", \"split1_test_score\", \"split2_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_dtree.best_params_)\n",
    "print(grid_dtree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = grid_dtree.best_estimator_\n",
    "pred = estimator.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 전처리\n",
    "- 결손값 처리(NaN, Null)\n",
    "    - Null 값이 적으면 평균값 등으로 대체한다.\n",
    "    - Null 값이 대부분이면 해당 피처를 드랍시킨다.\n",
    "    - Null 값이 일정 수준 이상이면, 해당 피처가 높은 중요도, 단순히 평균값으로 대체하면 왜곡이 심한 경우 등\n",
    "- 입력이 문자열인 경우\n",
    "    - 인코딩하여 숫자로 변환해야 한다.\n",
    "    - 카테고리형 피처 => 코드값으로(수박, 바나나, 딸기 분류 => 수박 = 0번, 바나나 = 1번, 딸기 = 2번)\n",
    "    - 텍스트형 피처 => 피처 백터와\n",
    "    - 불필요한 픽처 => 삭제\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 인코딩\n",
    "- 레이블 인코딩(Label encoding)\n",
    "    - 카테고리 피처를 코드형 숫자 값으로 변환한다.\n",
    "        - ex) TV : 1, 냉장고 : 2, 전자레인지 : 3\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items = [\"TV\", \"냉장고\", \"전자레인지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items) # TV는 몇번, 냉장고는 몇번 이런걸 만든다\n",
    "print(encoder.transform(items)) # items를 encoder.fit()에서 변경한 숫자로 만들어준다.\n",
    "print(encoder.classes_)\n",
    "print(encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) # 숫자를 다시 text로 변경한다.\n",
    "'''\n",
    "숫자 값으로 변환되지만 숫자는 의미는 없다.\n",
    "- 숫자의 특성이 반영되면 안된다.\n",
    "- 회귀 등에는 레이블 인코딩을 적용하면 안된다.\n",
    "    - 회귀는 숫자의 특성을 반영하기에 레이블 인코딩을 적용하면 안됨\n",
    "- 의사 결정 트리는 숫자의 특성이 반영되지 않으므로 괜찮다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 인코딩\n",
    "- 원-핫 인코딩(One Hot Encoding)\n",
    "    - 고유 값에 해당하는 컬럼만 1이고 나머지는 0이다.\n",
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items = [\"TV\", \"냉장고\", \"전자레인지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "labels = labels.reshape(-1, 1)\n",
    "\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "print(oh_labels.toarray())\n",
    "print(oh_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "판다스를 사용한 원 핫 인코딩\n",
    "'''\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"item\" : [\"TV\", \"냉장고\", \"전자레인지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]})\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 전처리\n",
    "- 피처 스케일링(Feature Scaling)\n",
    "    - 서로 다른 변수의 값 범위를 일정 수준으로 맞추는 작업이다.\n",
    "    - 표준화(가우시안 정규 분포)\n",
    "        - 피처 각각을 평균 0, 분산 1 가우시안 정규 분포로 변환한다.\n",
    "    - 정규화\n",
    "        - 각 피처의 크기를 통일하기 위해 크기를 변환한다.\n",
    "        - 거리 0 ~ 100km\n",
    "        - 금액 0 ~ 100,000,000,000원\n",
    "            - 거리와 금액을 0 ~ 1로 변환한다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터 전처리\n",
    "StandardScaler\n",
    "- 표준화를 지원하는 클래스이다.\n",
    "- 사이킷런의 서포트 벡터 머신, 선형 회귀, 로지스틱 회귀는 데이터가 가우시안 분포라고 가정하고 구현되었다.\n",
    "    - 즉, 사전에 데이터 표준화가 필요하다.\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "\n",
    "print(iris_df.mean())\n",
    "print(iris_df.var())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns=iris.feature_names)\n",
    "print(iris_df_scaled.mean())\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MinMaxScaler\n",
    "- 데이터 값을 0 ~ 1 범위로 변환한다.\n",
    "- 음수 값이 있으면 -1 ~ 1로 변환한다.\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print(iris_df_scaled.min())\n",
    "print(iris_df_scaled.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "StandardScaler, MinMaxScaler 적용 시 주의할 점\n",
    "- 학습 데이터로 만든 스케일링 정보를 그대로 테스트 데이터에 적용해야 한다.\n",
    "    - 안그러면, 학습 데이터와 테스트 데이터의 스케일링이 달라서 결과가 이상해진다.\n",
    "        - 학습 데이터로 fit(), transform()\n",
    "        - 테스트 데이터로 transform()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "잘못된 전처리\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "train_array = np.arange(0, 11).reshape(-1, 1) # train data는 1/10으로 스케일링\n",
    "test_array = np.arange(0, 6).reshape(-1, 1)   # test data는 1/5로 스케일링\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_array)\n",
    "train_scaled = scaler.transform(train_array)\n",
    "print(train_array.reshape(-1))\n",
    "print(train_scaled.reshape(-1))\n",
    "\n",
    "scaler.fit(test_array)\n",
    "test_scaled = scaler.transform(test_array)\n",
    "print(test_array.reshape(-1))\n",
    "print(test_scaled.reshape(-1))\n",
    "# 학습 데이터는 1/10으로 스케일링, 테스트 데이터는 1/5로 스케일링 되므로 이상하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n전체 데이터를 스케일링 변환한 후, 학습 / 테스트 데이터로 분리하는 것이 좋다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "전체 데이터를 스케일링 변환한 후, 학습 / 테스트 데이터로 분리하는 것이 좋다.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
